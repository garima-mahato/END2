# Assignment
---

1) Replace the embeddings of this session's code with GloVe embeddings

2) Compare your results with this session's code. 

3) Upload to a public GitHub repo and proceed to Session 10 Assignment Solutions where these questions are asked: 

> 1) Share the link to your README file's public repo for this assignment. Expecting a minimum 500-word write-up on your learnings.  Expecting you to compare your results with the code covered in the class. - 750 Points

> 2) Share the link to your main notebook with training logs - 250 Points

# Solution
---

**Task: English to Frensch Translation**


### Training Logs

```
6m 23s (- 89m 29s) (5000 6%) 3.4010
12m 43s (- 82m 41s) (10000 13%) 2.7494
19m 3s (- 76m 13s) (15000 20%) 2.3543
25m 24s (- 69m 51s) (20000 26%) 2.0915
31m 46s (- 63m 32s) (25000 33%) 1.8712
38m 7s (- 57m 10s) (30000 40%) 1.6453
44m 29s (- 50m 50s) (35000 46%) 1.5507
50m 55s (- 44m 33s) (40000 53%) 1.3937
57m 19s (- 38m 12s) (45000 60%) 1.2722
63m 44s (- 31m 52s) (50000 66%) 1.1702
```



## Teacher Forcing

**Teacher Forcing** is a technique to quickly and efficiently train recurrent neural network models that use the ground truth from a prior time step as input. In sequence prediction models, the output from the last time step y(t-1) as input for the model at the current time step X(t). It is used in tasks such as Machine Translation, Caption Generation, Text Summarization, etc.

The problem with this training approach is that since the model is in learning phase, the output generated by previous time step will not be accurate and so each successive steps would get affected. This is like learning subject A from a student who herself is learning subject A. This leads to slower convergence of the model and instability in the model.

Solution to this problem is instead of learning from a student, learning from teacher would give better esults. The teacher in this case would be the target values. Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network(y_hat(t)).

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/tf1.png)

Consider above example. Target is *Two people reading book*. Since the 2nd word is incorrectly predicted as *bird*, in absence of teacher forcing, wrong word will go as input to next step. In teacher forcing, gound tructh *people* goes as input. Thus, model will learn better in teacher forcig.

Pros:

Training with Teacher Forcing converges faster. This is because in the early stages of training, the predictions of the model are very bad. If we do not use Teacher Forcing, the hidden states of the model will be updated by a sequence of wrong predictions, errors will accumulate, and it is difficult for the model to learn from that.

Cons:

During inference, since there is usually no ground truth available, the model will need to feed its own previous prediction back to itself for the next prediction. Therefore there is a discrepancy between training and inference, and this might lead to poor model performance and instability. This is known as **Exposure Bias**.