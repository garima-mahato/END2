

# Assignment
---

1) Replace the embeddings of this session's code with GloVe embeddings

2) Compare your results with this session's code. 

3) Upload to a public GitHub repo and proceed to Session 10 Assignment Solutions where these questions are asked: 

> 1) Share the link to your README file's public repo for this assignment. Expecting a minimum 500-word write-up on your learnings.  Expecting you to compare your results with the code covered in the class. - 750 Points

> 2) Share the link to your main notebook with training logs - 250 Points

# Solution
---

**Task: English to French Translation using Glove Embeddings**

| GitHub Link | Google Colab Link |
|---|---|
|[Link to Code](https://github.com/garima-mahato/END2/blob/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/END2_Session10_END2_Translation_using_Seq2Seq_and_Attention.ipynb)|[Link to Code](https://githubtocolab.com/garima-mahato/END2/blob/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/END2_Session10_END2_Translation_using_Seq2Seq_and_Attention.ipynb)


<!--https://user-images.githubusercontent.com/52399940/126055430-25b66751-6f3a-46e3-af25-d22e8007439f.mp4-->

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/seq2seq_2.gif)



## Architecture


### Encoder

In encoder, an english sentence is given as input along with initial hidden state. The english input is converted into embeddings using Glove embedding layer. The embedded vector and initial hidden state are then passed to GRU which gives all hidden states as output. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder because each hidden state corresponds to a particular word in the sentence.

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/seq2seq_9.gif)

### Decoder

<!--https://user-images.githubusercontent.com/52399940/126055121-8056ae8e-ee73-4698-a0e5-b9fb93c1313c.mp4-->

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/attention_process1.gif)

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/attention_tensor_dance.gif)

## Training

### Teacher Forcing

**Teacher Forcing** is a training technique to quickly and efficiently train recurrent neural network models that use the ground truth from a prior time step as input. In sequence prediction models, the output from the last time step y(t-1) as input for the model at the current time step X(t). It is used in tasks such as Machine Translation, Caption Generation, Text Summarization, etc.

The problem with this training approach is that since the model is in learning phase, the output generated by previous time step will not be accurate and so each successive steps would get affected. This is like learning subject A from a student who herself is learning subject A. This leads to slower convergence of the model and instability in the model.

Solution to this problem is instead of learning from a student, learning from teacher would give better esults. The teacher in this case would be the target values. Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network(y_hat(t)).

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/tf1.png)

Consider above example. Target is *Two people reading book*. Since the 2nd word is incorrectly predicted as *bird*, in absence of teacher forcing, wrong word will go as input to next step. In teacher forcing, gound tructh *people* goes as input. Thus, model will learn better in teacher forcig.

Pros:

Training with Teacher Forcing converges faster. This is because in the early stages of training, the predictions of the model are very bad. If we do not use Teacher Forcing, the hidden states of the model will be updated by a sequence of wrong predictions, errors will accumulate, and it is difficult for the model to learn from that.

Cons:

During inference, since there is usually no ground truth available, the model will need to feed its own previous prediction back to itself for the next prediction. Therefore there is a discrepancy between training and inference, and this might lead to poor model performance and instability. This is known as **Exposure Bias**.

**Teacher Forcing with 50% probability is used here.**



### Training Logs

```
6m 27s (- 90m 21s) (5000 6%) 3.4356
12m 50s (- 83m 30s) (10000 13%) 2.7440
19m 16s (- 77m 6s) (15000 20%) 2.3890
25m 41s (- 70m 38s) (20000 26%) 2.1120
32m 6s (- 64m 13s) (25000 33%) 1.8726
38m 32s (- 57m 48s) (30000 40%) 1.6580
44m 57s (- 51m 22s) (35000 46%) 1.5425
51m 24s (- 44m 59s) (40000 53%) 1.3826
57m 49s (- 38m 33s) (45000 60%) 1.2583
64m 14s (- 32m 7s) (50000 66%) 1.1682
70m 39s (- 25m 41s) (55000 73%) 1.0912
77m 4s (- 19m 16s) (60000 80%) 1.0004
83m 29s (- 12m 50s) (65000 86%) 0.9606
89m 52s (- 6m 25s) (70000 93%) 0.9084
96m 12s (- 0m 0s) (75000 100%) 0.8534
```

### Visualization

**Attention Visualization**

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/att_is2.PNG)

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/att_vis3.PNG)

## Evaluation

```
> she is being blackmailed by him .
= il exerce sur elle du chantage .
< elle va beaucoup lui lui . <EOS>

> you re very religious aren t you ?
= vous etes tres religieuses n est ce pas ?
< vous etes tres religieux n est ce pas ? <EOS>

> he is on the team .
= il fait partie de l equipe .
< il est partie de la equipe . <EOS>

> you re the leader .
= c est vous la chef .
< vous etes la chef chef . <EOS>

> you are too young to travel alone .
= vous etes trop jeunes pour voyager seuls .
< vous etes trop jeune pour voyager seul . <EOS>

> you aren t supposed to swim here .
= tu n es pas cense nager ici .
< vous n etes pas censees nager ici . <EOS>

> he s no saint .
= il n est pas un saint .
< ce n est pas un saint . <EOS>

> you re a woman now .
= vous etes desormais une femme .
< vous etes une femme maintenant . <EOS>

> we re surprised .
= nous sommes surprises .
< nous sommes surpris . <EOS>

> we re open tomorrow .
= nous sommes ouverts demain .
< nous sommes ouverts demain . <EOS>
```

## Improvements

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/imp1.PNG)

1) LSTM and Bi-directional LSTMs can be used instead of RNN

2) Adam optimizer can be used instead of SGD
