

# Assignment
---

1) Replace the embeddings of this session's code with GloVe embeddings

2) Compare your results with this session's code. 

3) Upload to a public GitHub repo and proceed to Session 10 Assignment Solutions where these questions are asked: 

> 1) Share the link to your README file's public repo for this assignment. Expecting a minimum 500-word write-up on your learnings.  Expecting you to compare your results with the code covered in the class. - 750 Points

> 2) Share the link to your main notebook with training logs - 250 Points

# Solution
---

**Task: English to French Translation**

| GitHub Link | Google Colab Link |
|---|---|
|[Link to Code](https://github.com/garima-mahato/END2/blob/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/END2_Session10_END2_Translation_using_Seq2Seq_and_Attention.ipynb)|[Link to Code](https://githubtocolab.com/garima-mahato/END2/blob/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/END2_Session10_END2_Translation_using_Seq2Seq_and_Attention.ipynb)


<!--https://user-images.githubusercontent.com/52399940/126055430-25b66751-6f3a-46e3-af25-d22e8007439f.mp4-->

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/seq2seq_2.gif)



## Architecture



### Encoder

### Decoder

<!--https://user-images.githubusercontent.com/52399940/126055121-8056ae8e-ee73-4698-a0e5-b9fb93c1313c.mp4-->

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/attention_process1.gif)

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/attention_tensor_dance.gif)

### Training Logs

```
6m 27s (- 90m 21s) (5000 6%) 3.4356
12m 50s (- 83m 30s) (10000 13%) 2.7440
19m 16s (- 77m 6s) (15000 20%) 2.3890
25m 41s (- 70m 38s) (20000 26%) 2.1120
32m 6s (- 64m 13s) (25000 33%) 1.8726
38m 32s (- 57m 48s) (30000 40%) 1.6580
44m 57s (- 51m 22s) (35000 46%) 1.5425
51m 24s (- 44m 59s) (40000 53%) 1.3826
57m 49s (- 38m 33s) (45000 60%) 1.2583
64m 14s (- 32m 7s) (50000 66%) 1.1682
70m 39s (- 25m 41s) (55000 73%) 1.0912
77m 4s (- 19m 16s) (60000 80%) 1.0004
83m 29s (- 12m 50s) (65000 86%) 0.9606
89m 52s (- 6m 25s) (70000 93%) 0.9084
96m 12s (- 0m 0s) (75000 100%) 0.8534
```



## Teacher Forcing

**Teacher Forcing** is a technique to quickly and efficiently train recurrent neural network models that use the ground truth from a prior time step as input. In sequence prediction models, the output from the last time step y(t-1) as input for the model at the current time step X(t). It is used in tasks such as Machine Translation, Caption Generation, Text Summarization, etc.

The problem with this training approach is that since the model is in learning phase, the output generated by previous time step will not be accurate and so each successive steps would get affected. This is like learning subject A from a student who herself is learning subject A. This leads to slower convergence of the model and instability in the model.

Solution to this problem is instead of learning from a student, learning from teacher would give better esults. The teacher in this case would be the target values. Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network(y_hat(t)).

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/tf1.png)

Consider above example. Target is *Two people reading book*. Since the 2nd word is incorrectly predicted as *bird*, in absence of teacher forcing, wrong word will go as input to next step. In teacher forcing, gound tructh *people* goes as input. Thus, model will learn better in teacher forcig.

Pros:

Training with Teacher Forcing converges faster. This is because in the early stages of training, the predictions of the model are very bad. If we do not use Teacher Forcing, the hidden states of the model will be updated by a sequence of wrong predictions, errors will accumulate, and it is difficult for the model to learn from that.

Cons:

During inference, since there is usually no ground truth available, the model will need to feed its own previous prediction back to itself for the next prediction. Therefore there is a discrepancy between training and inference, and this might lead to poor model performance and instability. This is known as **Exposure Bias**.

**Teacher Forcing with 50% probability is used here.**

## Improvements

![](https://raw.githubusercontent.com/garima-mahato/END2/main/Session10-3rdHandson-LanguageTranslationusingSeq2SeqwithAttention/assets/imp1.PNG)

1) LSTM and Bi-directional LSTMs can be used instead of RNN

2) Adam optimizer can be used instead of SGD
