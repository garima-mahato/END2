{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END2_Session10_END2 Translation using Seq2Seq and Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxCla6HFCtRH",
        "outputId": "06d7e73d-a699-401f-8ce8-8a22b3dc3c83"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install bcolz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2650802 sha256=b60ae872791af2909c4ae7845e3c52da30240516dc6bfd29f1bd0a1c4f44a48f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOr51ZdbymLp",
        "outputId": "48024879-00ca-4db9-bfda-95128b6df953"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Sm1RVyDOVE"
      },
      "source": [
        "\n",
        "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "*******************************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "[PyTorch Source](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
        "write our own classes and functions to preprocess the data to do our NLP\n",
        "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
        "learn how `torchtext` can handle much of this preprocessing for you in the\n",
        "three tutorials immediately following this one.\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  [Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html): A 60 Minute Blitz to get started with PyTorch in general\n",
        "-  [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) for a wide and deep overview\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "-  [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "-  [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)\n",
        "\n",
        "\n",
        "\n",
        "![Image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EEWOCJLDD-E"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import bcolz  # to process the data from Glove File \n",
        "import pickle # to dump and load pretrained glove vectors \n",
        "import copy   # to make deepcopy of python lists and dictionaries\n",
        "import operator\n",
        "import numpy as np\n",
        "from pandas import DataFrame # to visualize the glove word embeddings in form of DataFrame\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30yEbATJDXCF"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "[This question on Open Data Stack\n",
        "Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)\n",
        "pointed me/him to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_EPl_QFDUSE",
        "outputId": "bbe122b6-dd7c-4853-c75d-6265431f712a"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-18 03:49:58--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.84.206.26, 99.84.206.82, 99.84.206.63, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.84.206.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-07-18 03:49:59 (63.9 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugKjISLqznF-"
      },
      "source": [
        "Download  and Unzip Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVn2PBI6zdRg",
        "outputId": "51de98a7-0622-4299-d2c6-829ceef63024"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 17:47:40--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-07-17 17:47:40--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-07-17 17:47:40--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 40s  \n",
            "\n",
            "2021-07-17 17:50:20 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ4g0l2aRYAr"
      },
      "source": [
        "#### Loading and Saving Glove Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkdS9ZtC1oXB"
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir=f'/gdrive/MyDrive/TSAI_END2/Session10/6B.300.dat', mode='w')\n",
        "\n",
        "with open(f'/content/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "    \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400001, 300)), rootdir=f'/gdrive/MyDrive/TSAI_END2/Session10/6B.300.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_idx.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdPjq-hd8UYW"
      },
      "source": [
        "pickle.dump(words, open(f'./glove.6B.300_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'./glove.6B.300_idx.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpeptdfO9tro",
        "outputId": "21418792-b162-47e1-de1d-db515d55057d"
      },
      "source": [
        "!ls '/content/drive/MyDrive/TSAI_END2/Session10'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6B.300.dat  glove.6B.300_idx.pkl    glove.6B.50_idx.pkl\n",
            "6B.50.dat   glove.6B.300_words.pkl  glove.6B.50_words.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIKUn1lcu2a"
      },
      "source": [
        "#### Load saved vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UilzgUKWRfWZ"
      },
      "source": [
        "vectors = bcolz.open(f'/content/drive/MyDrive/TSAI_END2/Session10/6B.300.dat')[:]\n",
        "words = pickle.load(open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCBGOD0XT2bP"
      },
      "source": [
        "glove_df = DataFrame(vectors, columns=range(1,301), index=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyUmAVY4Uxrt"
      },
      "source": [
        "#### Visualize the Glove Word Embeddings by making a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "B259PP29T8Ro",
        "outputId": "9b562a98-e75c-4ada-bfba-269e7a9fc0a8"
      },
      "source": [
        "glove_df[200:210]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>according</th>\n",
              "      <td>-0.279610</td>\n",
              "      <td>0.137320</td>\n",
              "      <td>0.043514</td>\n",
              "      <td>0.333020</td>\n",
              "      <td>-0.168460</td>\n",
              "      <td>0.067218</td>\n",
              "      <td>-0.166420</td>\n",
              "      <td>0.157180</td>\n",
              "      <td>-0.121350</td>\n",
              "      <td>-1.73860</td>\n",
              "      <td>-0.024858</td>\n",
              "      <td>-0.265710</td>\n",
              "      <td>0.175370</td>\n",
              "      <td>0.173250</td>\n",
              "      <td>-0.002423</td>\n",
              "      <td>0.159280</td>\n",
              "      <td>-0.186030</td>\n",
              "      <td>0.251630</td>\n",
              "      <td>-0.386520</td>\n",
              "      <td>-0.336200</td>\n",
              "      <td>0.126930</td>\n",
              "      <td>0.073719</td>\n",
              "      <td>0.249740</td>\n",
              "      <td>0.456310</td>\n",
              "      <td>-0.20157</td>\n",
              "      <td>0.017940</td>\n",
              "      <td>-0.085641</td>\n",
              "      <td>0.082276</td>\n",
              "      <td>0.202490</td>\n",
              "      <td>-0.137970</td>\n",
              "      <td>0.040790</td>\n",
              "      <td>0.547840</td>\n",
              "      <td>-0.041509</td>\n",
              "      <td>0.19313</td>\n",
              "      <td>-0.80545</td>\n",
              "      <td>-0.226530</td>\n",
              "      <td>0.200270</td>\n",
              "      <td>-0.039198</td>\n",
              "      <td>-0.175200</td>\n",
              "      <td>-0.179190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063811</td>\n",
              "      <td>0.47207</td>\n",
              "      <td>0.037923</td>\n",
              "      <td>-0.361250</td>\n",
              "      <td>0.209150</td>\n",
              "      <td>-0.245690</td>\n",
              "      <td>0.258970</td>\n",
              "      <td>-0.060094</td>\n",
              "      <td>-0.137910</td>\n",
              "      <td>-0.233440</td>\n",
              "      <td>-0.192400</td>\n",
              "      <td>-0.188730</td>\n",
              "      <td>0.157660</td>\n",
              "      <td>0.178660</td>\n",
              "      <td>0.11679</td>\n",
              "      <td>-0.192830</td>\n",
              "      <td>-1.9656</td>\n",
              "      <td>-0.10005</td>\n",
              "      <td>0.48721</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>0.010740</td>\n",
              "      <td>-0.108050</td>\n",
              "      <td>0.054460</td>\n",
              "      <td>-0.356230</td>\n",
              "      <td>0.142410</td>\n",
              "      <td>0.184360</td>\n",
              "      <td>-0.032794</td>\n",
              "      <td>-0.066567</td>\n",
              "      <td>-0.16291</td>\n",
              "      <td>0.345220</td>\n",
              "      <td>-0.013469</td>\n",
              "      <td>-0.219000</td>\n",
              "      <td>0.037849</td>\n",
              "      <td>0.155860</td>\n",
              "      <td>0.442500</td>\n",
              "      <td>0.433770</td>\n",
              "      <td>-0.192290</td>\n",
              "      <td>0.390080</td>\n",
              "      <td>-0.324610</td>\n",
              "      <td>-0.01826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>several</th>\n",
              "      <td>-0.278830</td>\n",
              "      <td>0.298990</td>\n",
              "      <td>-0.092609</td>\n",
              "      <td>-0.149200</td>\n",
              "      <td>0.102150</td>\n",
              "      <td>-0.187240</td>\n",
              "      <td>-0.057946</td>\n",
              "      <td>0.050455</td>\n",
              "      <td>-0.045006</td>\n",
              "      <td>-1.36610</td>\n",
              "      <td>-0.197030</td>\n",
              "      <td>0.145630</td>\n",
              "      <td>-0.187190</td>\n",
              "      <td>0.109940</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>-0.108720</td>\n",
              "      <td>-0.462160</td>\n",
              "      <td>0.170320</td>\n",
              "      <td>0.117040</td>\n",
              "      <td>-0.029810</td>\n",
              "      <td>-0.005754</td>\n",
              "      <td>-0.049542</td>\n",
              "      <td>0.319010</td>\n",
              "      <td>0.118340</td>\n",
              "      <td>-0.32802</td>\n",
              "      <td>-0.151250</td>\n",
              "      <td>-0.486250</td>\n",
              "      <td>0.179170</td>\n",
              "      <td>-0.274930</td>\n",
              "      <td>0.317760</td>\n",
              "      <td>0.235180</td>\n",
              "      <td>0.155060</td>\n",
              "      <td>0.100110</td>\n",
              "      <td>0.23292</td>\n",
              "      <td>-0.55807</td>\n",
              "      <td>-0.352460</td>\n",
              "      <td>0.081194</td>\n",
              "      <td>0.386270</td>\n",
              "      <td>-0.094630</td>\n",
              "      <td>-0.074292</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017501</td>\n",
              "      <td>0.34991</td>\n",
              "      <td>0.358650</td>\n",
              "      <td>-0.209420</td>\n",
              "      <td>0.204730</td>\n",
              "      <td>-0.258020</td>\n",
              "      <td>-0.770900</td>\n",
              "      <td>-0.010413</td>\n",
              "      <td>0.049296</td>\n",
              "      <td>0.006965</td>\n",
              "      <td>0.144020</td>\n",
              "      <td>0.279530</td>\n",
              "      <td>-0.190090</td>\n",
              "      <td>0.130970</td>\n",
              "      <td>0.27707</td>\n",
              "      <td>0.009039</td>\n",
              "      <td>-2.6111</td>\n",
              "      <td>-0.17615</td>\n",
              "      <td>0.25067</td>\n",
              "      <td>0.108000</td>\n",
              "      <td>-0.391450</td>\n",
              "      <td>0.023849</td>\n",
              "      <td>0.234420</td>\n",
              "      <td>0.049562</td>\n",
              "      <td>-0.275360</td>\n",
              "      <td>0.289470</td>\n",
              "      <td>-0.143550</td>\n",
              "      <td>0.219270</td>\n",
              "      <td>0.46774</td>\n",
              "      <td>-0.174360</td>\n",
              "      <td>-0.064541</td>\n",
              "      <td>-0.324480</td>\n",
              "      <td>-0.180430</td>\n",
              "      <td>0.003164</td>\n",
              "      <td>-0.176020</td>\n",
              "      <td>0.478750</td>\n",
              "      <td>-0.237680</td>\n",
              "      <td>-0.224160</td>\n",
              "      <td>-0.366290</td>\n",
              "      <td>-0.21762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>court</th>\n",
              "      <td>0.294920</td>\n",
              "      <td>-0.274790</td>\n",
              "      <td>-0.616680</td>\n",
              "      <td>-0.055869</td>\n",
              "      <td>0.385780</td>\n",
              "      <td>-0.582630</td>\n",
              "      <td>0.730860</td>\n",
              "      <td>-0.119370</td>\n",
              "      <td>-0.041382</td>\n",
              "      <td>-1.42530</td>\n",
              "      <td>-0.305880</td>\n",
              "      <td>-0.056891</td>\n",
              "      <td>-0.195040</td>\n",
              "      <td>0.198430</td>\n",
              "      <td>-0.554840</td>\n",
              "      <td>0.283550</td>\n",
              "      <td>-0.537580</td>\n",
              "      <td>-0.305180</td>\n",
              "      <td>0.569070</td>\n",
              "      <td>-0.239610</td>\n",
              "      <td>0.186620</td>\n",
              "      <td>-0.184960</td>\n",
              "      <td>0.440740</td>\n",
              "      <td>-0.083826</td>\n",
              "      <td>-0.25820</td>\n",
              "      <td>-0.044409</td>\n",
              "      <td>0.139150</td>\n",
              "      <td>-0.541320</td>\n",
              "      <td>-0.460620</td>\n",
              "      <td>0.581210</td>\n",
              "      <td>-0.022778</td>\n",
              "      <td>0.173250</td>\n",
              "      <td>0.178510</td>\n",
              "      <td>0.29736</td>\n",
              "      <td>-0.82185</td>\n",
              "      <td>-0.389630</td>\n",
              "      <td>0.094130</td>\n",
              "      <td>-0.205660</td>\n",
              "      <td>-0.772460</td>\n",
              "      <td>0.181290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189250</td>\n",
              "      <td>-0.16735</td>\n",
              "      <td>0.412260</td>\n",
              "      <td>0.142210</td>\n",
              "      <td>0.170160</td>\n",
              "      <td>-0.354630</td>\n",
              "      <td>0.061339</td>\n",
              "      <td>0.343660</td>\n",
              "      <td>-0.046048</td>\n",
              "      <td>-1.083400</td>\n",
              "      <td>0.439800</td>\n",
              "      <td>-0.694310</td>\n",
              "      <td>0.207420</td>\n",
              "      <td>0.119580</td>\n",
              "      <td>0.19484</td>\n",
              "      <td>-0.004723</td>\n",
              "      <td>-2.1147</td>\n",
              "      <td>0.18858</td>\n",
              "      <td>1.13600</td>\n",
              "      <td>0.931650</td>\n",
              "      <td>0.113970</td>\n",
              "      <td>0.024541</td>\n",
              "      <td>0.015915</td>\n",
              "      <td>0.209480</td>\n",
              "      <td>-0.301930</td>\n",
              "      <td>-0.062889</td>\n",
              "      <td>0.281870</td>\n",
              "      <td>0.570090</td>\n",
              "      <td>-0.53056</td>\n",
              "      <td>0.285680</td>\n",
              "      <td>0.437920</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.241620</td>\n",
              "      <td>0.506370</td>\n",
              "      <td>0.290290</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.136190</td>\n",
              "      <td>-0.207100</td>\n",
              "      <td>0.015589</td>\n",
              "      <td>0.17391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>say</th>\n",
              "      <td>0.056090</td>\n",
              "      <td>0.200350</td>\n",
              "      <td>-0.136570</td>\n",
              "      <td>0.057160</td>\n",
              "      <td>-0.127080</td>\n",
              "      <td>-0.201790</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.261590</td>\n",
              "      <td>-0.052328</td>\n",
              "      <td>-1.75730</td>\n",
              "      <td>0.076761</td>\n",
              "      <td>-0.088249</td>\n",
              "      <td>-0.053148</td>\n",
              "      <td>0.581910</td>\n",
              "      <td>0.458550</td>\n",
              "      <td>0.270420</td>\n",
              "      <td>0.020048</td>\n",
              "      <td>0.052781</td>\n",
              "      <td>0.235690</td>\n",
              "      <td>0.287740</td>\n",
              "      <td>0.162650</td>\n",
              "      <td>0.162660</td>\n",
              "      <td>0.349910</td>\n",
              "      <td>-0.291750</td>\n",
              "      <td>-0.67895</td>\n",
              "      <td>0.058927</td>\n",
              "      <td>0.219020</td>\n",
              "      <td>-0.378070</td>\n",
              "      <td>-0.041854</td>\n",
              "      <td>-0.118250</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>0.572670</td>\n",
              "      <td>-0.604510</td>\n",
              "      <td>-0.14544</td>\n",
              "      <td>-0.63381</td>\n",
              "      <td>0.081092</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.210160</td>\n",
              "      <td>0.180770</td>\n",
              "      <td>-0.514370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.145000</td>\n",
              "      <td>-0.19456</td>\n",
              "      <td>-0.159280</td>\n",
              "      <td>0.204040</td>\n",
              "      <td>-0.080843</td>\n",
              "      <td>0.223510</td>\n",
              "      <td>-0.135650</td>\n",
              "      <td>-0.096678</td>\n",
              "      <td>-0.085022</td>\n",
              "      <td>0.077191</td>\n",
              "      <td>0.182990</td>\n",
              "      <td>0.298280</td>\n",
              "      <td>0.107660</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>0.36590</td>\n",
              "      <td>0.197160</td>\n",
              "      <td>-2.4170</td>\n",
              "      <td>0.10348</td>\n",
              "      <td>0.37408</td>\n",
              "      <td>-0.036688</td>\n",
              "      <td>0.004247</td>\n",
              "      <td>0.044540</td>\n",
              "      <td>-0.124100</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.063308</td>\n",
              "      <td>0.182960</td>\n",
              "      <td>0.269000</td>\n",
              "      <td>-0.223500</td>\n",
              "      <td>-0.17115</td>\n",
              "      <td>0.082492</td>\n",
              "      <td>0.189930</td>\n",
              "      <td>-0.544910</td>\n",
              "      <td>0.171560</td>\n",
              "      <td>-0.017652</td>\n",
              "      <td>0.174040</td>\n",
              "      <td>-0.603500</td>\n",
              "      <td>-0.107820</td>\n",
              "      <td>-0.501320</td>\n",
              "      <td>-0.021014</td>\n",
              "      <td>0.44665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>around</th>\n",
              "      <td>-0.469570</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>-0.030859</td>\n",
              "      <td>-0.107720</td>\n",
              "      <td>-0.025968</td>\n",
              "      <td>0.408180</td>\n",
              "      <td>0.218770</td>\n",
              "      <td>0.382240</td>\n",
              "      <td>0.105420</td>\n",
              "      <td>-1.64180</td>\n",
              "      <td>-0.113610</td>\n",
              "      <td>0.291120</td>\n",
              "      <td>0.057045</td>\n",
              "      <td>0.199720</td>\n",
              "      <td>0.291610</td>\n",
              "      <td>0.366450</td>\n",
              "      <td>-0.456570</td>\n",
              "      <td>0.454240</td>\n",
              "      <td>-0.093902</td>\n",
              "      <td>0.331720</td>\n",
              "      <td>0.299060</td>\n",
              "      <td>0.338220</td>\n",
              "      <td>0.281110</td>\n",
              "      <td>-0.142310</td>\n",
              "      <td>0.18127</td>\n",
              "      <td>-0.004856</td>\n",
              "      <td>-0.112540</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>-0.254640</td>\n",
              "      <td>0.506790</td>\n",
              "      <td>0.098070</td>\n",
              "      <td>0.163230</td>\n",
              "      <td>-0.481590</td>\n",
              "      <td>0.55863</td>\n",
              "      <td>-0.67272</td>\n",
              "      <td>-0.025186</td>\n",
              "      <td>0.163960</td>\n",
              "      <td>0.465680</td>\n",
              "      <td>-0.088058</td>\n",
              "      <td>-0.114930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850180</td>\n",
              "      <td>0.27484</td>\n",
              "      <td>0.119640</td>\n",
              "      <td>-0.116100</td>\n",
              "      <td>0.069383</td>\n",
              "      <td>-0.413810</td>\n",
              "      <td>-0.262600</td>\n",
              "      <td>-0.124260</td>\n",
              "      <td>0.209350</td>\n",
              "      <td>-0.061349</td>\n",
              "      <td>-0.152320</td>\n",
              "      <td>0.141830</td>\n",
              "      <td>0.320900</td>\n",
              "      <td>0.116200</td>\n",
              "      <td>0.21903</td>\n",
              "      <td>-0.179330</td>\n",
              "      <td>-2.1543</td>\n",
              "      <td>-0.41776</td>\n",
              "      <td>0.08200</td>\n",
              "      <td>0.206420</td>\n",
              "      <td>-0.344030</td>\n",
              "      <td>-0.122760</td>\n",
              "      <td>-0.075667</td>\n",
              "      <td>-0.065374</td>\n",
              "      <td>-0.269220</td>\n",
              "      <td>0.390050</td>\n",
              "      <td>0.199480</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.22481</td>\n",
              "      <td>-0.043952</td>\n",
              "      <td>0.573730</td>\n",
              "      <td>-0.227680</td>\n",
              "      <td>-0.388810</td>\n",
              "      <td>0.240210</td>\n",
              "      <td>-0.081779</td>\n",
              "      <td>0.140490</td>\n",
              "      <td>0.036649</td>\n",
              "      <td>0.184050</td>\n",
              "      <td>-0.110230</td>\n",
              "      <td>-0.31083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>foreign</th>\n",
              "      <td>-0.095487</td>\n",
              "      <td>-0.209960</td>\n",
              "      <td>-0.199050</td>\n",
              "      <td>0.417170</td>\n",
              "      <td>-0.034925</td>\n",
              "      <td>-0.194490</td>\n",
              "      <td>-0.381970</td>\n",
              "      <td>0.391020</td>\n",
              "      <td>-0.281870</td>\n",
              "      <td>-2.79180</td>\n",
              "      <td>0.354780</td>\n",
              "      <td>0.162590</td>\n",
              "      <td>-0.333660</td>\n",
              "      <td>0.025810</td>\n",
              "      <td>-0.335900</td>\n",
              "      <td>-0.808810</td>\n",
              "      <td>0.071127</td>\n",
              "      <td>0.015254</td>\n",
              "      <td>-0.131030</td>\n",
              "      <td>-0.122410</td>\n",
              "      <td>-0.198780</td>\n",
              "      <td>0.070378</td>\n",
              "      <td>0.983250</td>\n",
              "      <td>-0.055934</td>\n",
              "      <td>-0.30992</td>\n",
              "      <td>0.323390</td>\n",
              "      <td>0.192530</td>\n",
              "      <td>0.245240</td>\n",
              "      <td>-0.130560</td>\n",
              "      <td>0.260320</td>\n",
              "      <td>-0.476990</td>\n",
              "      <td>-0.203560</td>\n",
              "      <td>0.213620</td>\n",
              "      <td>0.04692</td>\n",
              "      <td>-0.89056</td>\n",
              "      <td>-0.688440</td>\n",
              "      <td>0.192790</td>\n",
              "      <td>-0.446870</td>\n",
              "      <td>0.116330</td>\n",
              "      <td>0.271760</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.226100</td>\n",
              "      <td>-0.40134</td>\n",
              "      <td>0.082960</td>\n",
              "      <td>0.521950</td>\n",
              "      <td>0.833800</td>\n",
              "      <td>-0.722610</td>\n",
              "      <td>-0.072281</td>\n",
              "      <td>-0.001010</td>\n",
              "      <td>-0.394940</td>\n",
              "      <td>0.068606</td>\n",
              "      <td>-0.099127</td>\n",
              "      <td>0.304630</td>\n",
              "      <td>-0.454250</td>\n",
              "      <td>-0.055678</td>\n",
              "      <td>-0.37260</td>\n",
              "      <td>0.151170</td>\n",
              "      <td>-1.6924</td>\n",
              "      <td>-0.41482</td>\n",
              "      <td>0.40577</td>\n",
              "      <td>-0.217920</td>\n",
              "      <td>0.402590</td>\n",
              "      <td>-0.233630</td>\n",
              "      <td>0.250910</td>\n",
              "      <td>-0.123690</td>\n",
              "      <td>0.548910</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>0.305510</td>\n",
              "      <td>0.32427</td>\n",
              "      <td>0.134810</td>\n",
              "      <td>-0.037153</td>\n",
              "      <td>-0.541010</td>\n",
              "      <td>0.109550</td>\n",
              "      <td>-0.251420</td>\n",
              "      <td>0.260630</td>\n",
              "      <td>0.715130</td>\n",
              "      <td>-0.140730</td>\n",
              "      <td>0.043425</td>\n",
              "      <td>-0.902620</td>\n",
              "      <td>0.49778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.441890</td>\n",
              "      <td>0.242480</td>\n",
              "      <td>-0.229970</td>\n",
              "      <td>0.172260</td>\n",
              "      <td>-0.096344</td>\n",
              "      <td>0.136520</td>\n",
              "      <td>0.039942</td>\n",
              "      <td>0.120760</td>\n",
              "      <td>0.389680</td>\n",
              "      <td>-0.96754</td>\n",
              "      <td>0.291700</td>\n",
              "      <td>0.275010</td>\n",
              "      <td>-0.175220</td>\n",
              "      <td>0.428170</td>\n",
              "      <td>0.012163</td>\n",
              "      <td>-0.045286</td>\n",
              "      <td>-0.129230</td>\n",
              "      <td>-0.029375</td>\n",
              "      <td>0.102550</td>\n",
              "      <td>-0.350450</td>\n",
              "      <td>-0.212720</td>\n",
              "      <td>-0.130870</td>\n",
              "      <td>-0.012698</td>\n",
              "      <td>0.092539</td>\n",
              "      <td>-0.06211</td>\n",
              "      <td>0.190630</td>\n",
              "      <td>-0.344460</td>\n",
              "      <td>-0.141980</td>\n",
              "      <td>0.009539</td>\n",
              "      <td>-0.286660</td>\n",
              "      <td>-0.187720</td>\n",
              "      <td>0.611720</td>\n",
              "      <td>0.137120</td>\n",
              "      <td>0.17352</td>\n",
              "      <td>-1.03470</td>\n",
              "      <td>-0.230870</td>\n",
              "      <td>0.407430</td>\n",
              "      <td>0.433120</td>\n",
              "      <td>-0.360320</td>\n",
              "      <td>0.036961</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279180</td>\n",
              "      <td>0.04196</td>\n",
              "      <td>0.064526</td>\n",
              "      <td>-0.200040</td>\n",
              "      <td>0.295430</td>\n",
              "      <td>-0.187930</td>\n",
              "      <td>0.192390</td>\n",
              "      <td>-0.072376</td>\n",
              "      <td>0.274850</td>\n",
              "      <td>0.285530</td>\n",
              "      <td>-0.097114</td>\n",
              "      <td>0.004545</td>\n",
              "      <td>0.148490</td>\n",
              "      <td>0.090044</td>\n",
              "      <td>0.13226</td>\n",
              "      <td>0.028023</td>\n",
              "      <td>-1.9839</td>\n",
              "      <td>-0.42138</td>\n",
              "      <td>-0.21883</td>\n",
              "      <td>0.160240</td>\n",
              "      <td>0.032295</td>\n",
              "      <td>-0.058029</td>\n",
              "      <td>0.103180</td>\n",
              "      <td>0.123430</td>\n",
              "      <td>-0.168160</td>\n",
              "      <td>0.682780</td>\n",
              "      <td>-0.516350</td>\n",
              "      <td>-0.147090</td>\n",
              "      <td>0.12471</td>\n",
              "      <td>0.165170</td>\n",
              "      <td>0.070977</td>\n",
              "      <td>-0.389240</td>\n",
              "      <td>-0.269190</td>\n",
              "      <td>-0.342620</td>\n",
              "      <td>0.324960</td>\n",
              "      <td>0.279240</td>\n",
              "      <td>-0.266480</td>\n",
              "      <td>-0.776380</td>\n",
              "      <td>-0.054478</td>\n",
              "      <td>-0.13019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>until</th>\n",
              "      <td>0.045036</td>\n",
              "      <td>-0.162020</td>\n",
              "      <td>0.401750</td>\n",
              "      <td>-0.032719</td>\n",
              "      <td>0.157200</td>\n",
              "      <td>-0.201640</td>\n",
              "      <td>0.112720</td>\n",
              "      <td>-0.183880</td>\n",
              "      <td>0.233380</td>\n",
              "      <td>-1.86600</td>\n",
              "      <td>-0.240630</td>\n",
              "      <td>-0.107330</td>\n",
              "      <td>0.321970</td>\n",
              "      <td>-0.139860</td>\n",
              "      <td>-0.089932</td>\n",
              "      <td>0.090911</td>\n",
              "      <td>0.179980</td>\n",
              "      <td>-0.082335</td>\n",
              "      <td>0.108850</td>\n",
              "      <td>0.146420</td>\n",
              "      <td>0.186290</td>\n",
              "      <td>0.090300</td>\n",
              "      <td>0.239850</td>\n",
              "      <td>-0.092648</td>\n",
              "      <td>-0.22982</td>\n",
              "      <td>-0.096040</td>\n",
              "      <td>-0.224780</td>\n",
              "      <td>-0.145510</td>\n",
              "      <td>-0.636020</td>\n",
              "      <td>0.010182</td>\n",
              "      <td>0.195490</td>\n",
              "      <td>0.153900</td>\n",
              "      <td>0.271010</td>\n",
              "      <td>0.30660</td>\n",
              "      <td>-0.77808</td>\n",
              "      <td>-0.012936</td>\n",
              "      <td>-0.094754</td>\n",
              "      <td>0.559800</td>\n",
              "      <td>-0.091743</td>\n",
              "      <td>0.416900</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.444470</td>\n",
              "      <td>-0.34940</td>\n",
              "      <td>-0.176270</td>\n",
              "      <td>-0.164460</td>\n",
              "      <td>-0.250770</td>\n",
              "      <td>-0.295420</td>\n",
              "      <td>0.013784</td>\n",
              "      <td>-0.373670</td>\n",
              "      <td>0.083342</td>\n",
              "      <td>-0.074837</td>\n",
              "      <td>-0.108870</td>\n",
              "      <td>0.145720</td>\n",
              "      <td>-0.053223</td>\n",
              "      <td>0.225040</td>\n",
              "      <td>0.49595</td>\n",
              "      <td>0.014406</td>\n",
              "      <td>-2.0235</td>\n",
              "      <td>-0.39581</td>\n",
              "      <td>0.16585</td>\n",
              "      <td>0.170420</td>\n",
              "      <td>-0.327020</td>\n",
              "      <td>-0.268010</td>\n",
              "      <td>-0.005349</td>\n",
              "      <td>0.118980</td>\n",
              "      <td>0.195720</td>\n",
              "      <td>0.491120</td>\n",
              "      <td>0.041823</td>\n",
              "      <td>0.249410</td>\n",
              "      <td>-0.36564</td>\n",
              "      <td>-0.020910</td>\n",
              "      <td>-0.197260</td>\n",
              "      <td>-0.091085</td>\n",
              "      <td>0.112990</td>\n",
              "      <td>-0.275180</td>\n",
              "      <td>-0.317930</td>\n",
              "      <td>0.384110</td>\n",
              "      <td>-0.136790</td>\n",
              "      <td>-0.506100</td>\n",
              "      <td>0.070201</td>\n",
              "      <td>0.31148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>-0.330680</td>\n",
              "      <td>0.174590</td>\n",
              "      <td>-0.325450</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.125360</td>\n",
              "      <td>-0.074719</td>\n",
              "      <td>-0.046026</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>-0.045669</td>\n",
              "      <td>-1.33110</td>\n",
              "      <td>0.409900</td>\n",
              "      <td>0.419970</td>\n",
              "      <td>0.073290</td>\n",
              "      <td>-0.034584</td>\n",
              "      <td>-0.325820</td>\n",
              "      <td>0.022083</td>\n",
              "      <td>0.067834</td>\n",
              "      <td>-0.207580</td>\n",
              "      <td>-0.048283</td>\n",
              "      <td>0.091015</td>\n",
              "      <td>0.089587</td>\n",
              "      <td>-0.046688</td>\n",
              "      <td>0.185640</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.24787</td>\n",
              "      <td>0.058006</td>\n",
              "      <td>-0.097955</td>\n",
              "      <td>-0.160070</td>\n",
              "      <td>-0.260060</td>\n",
              "      <td>0.204210</td>\n",
              "      <td>0.096925</td>\n",
              "      <td>0.307040</td>\n",
              "      <td>-0.025169</td>\n",
              "      <td>0.62919</td>\n",
              "      <td>-0.98415</td>\n",
              "      <td>0.038619</td>\n",
              "      <td>-0.375850</td>\n",
              "      <td>0.301880</td>\n",
              "      <td>-0.263370</td>\n",
              "      <td>0.428480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.228250</td>\n",
              "      <td>-0.27699</td>\n",
              "      <td>0.250080</td>\n",
              "      <td>0.067117</td>\n",
              "      <td>-0.027048</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.335750</td>\n",
              "      <td>0.060869</td>\n",
              "      <td>0.519090</td>\n",
              "      <td>-0.048897</td>\n",
              "      <td>-0.067377</td>\n",
              "      <td>-0.064250</td>\n",
              "      <td>0.609530</td>\n",
              "      <td>0.639080</td>\n",
              "      <td>-0.14792</td>\n",
              "      <td>0.170250</td>\n",
              "      <td>-2.1690</td>\n",
              "      <td>0.17169</td>\n",
              "      <td>0.41858</td>\n",
              "      <td>-0.148040</td>\n",
              "      <td>0.350980</td>\n",
              "      <td>-0.218970</td>\n",
              "      <td>-0.295720</td>\n",
              "      <td>-0.060498</td>\n",
              "      <td>-0.574690</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>-0.441010</td>\n",
              "      <td>0.023605</td>\n",
              "      <td>-0.14858</td>\n",
              "      <td>0.029384</td>\n",
              "      <td>-0.089098</td>\n",
              "      <td>-0.192170</td>\n",
              "      <td>-0.269420</td>\n",
              "      <td>0.147910</td>\n",
              "      <td>0.145190</td>\n",
              "      <td>0.973710</td>\n",
              "      <td>-0.121230</td>\n",
              "      <td>-0.400650</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>0.54662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>political</th>\n",
              "      <td>0.561610</td>\n",
              "      <td>-0.114750</td>\n",
              "      <td>0.421820</td>\n",
              "      <td>-0.146580</td>\n",
              "      <td>0.559030</td>\n",
              "      <td>-0.055492</td>\n",
              "      <td>-0.047198</td>\n",
              "      <td>-0.180620</td>\n",
              "      <td>0.451410</td>\n",
              "      <td>-2.19460</td>\n",
              "      <td>-0.071853</td>\n",
              "      <td>0.122400</td>\n",
              "      <td>0.246660</td>\n",
              "      <td>-0.122470</td>\n",
              "      <td>0.248140</td>\n",
              "      <td>-0.153350</td>\n",
              "      <td>0.152790</td>\n",
              "      <td>0.929340</td>\n",
              "      <td>0.057485</td>\n",
              "      <td>0.161260</td>\n",
              "      <td>0.230990</td>\n",
              "      <td>0.238820</td>\n",
              "      <td>0.450830</td>\n",
              "      <td>0.219870</td>\n",
              "      <td>-0.58242</td>\n",
              "      <td>0.254090</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>0.317270</td>\n",
              "      <td>0.017905</td>\n",
              "      <td>-0.093428</td>\n",
              "      <td>-0.191410</td>\n",
              "      <td>-0.020396</td>\n",
              "      <td>-0.486050</td>\n",
              "      <td>0.27129</td>\n",
              "      <td>-0.61117</td>\n",
              "      <td>0.149090</td>\n",
              "      <td>0.121830</td>\n",
              "      <td>0.669370</td>\n",
              "      <td>-0.018887</td>\n",
              "      <td>-0.091606</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083122</td>\n",
              "      <td>-0.22120</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.199290</td>\n",
              "      <td>0.415630</td>\n",
              "      <td>-0.168280</td>\n",
              "      <td>-0.054537</td>\n",
              "      <td>0.437520</td>\n",
              "      <td>0.005782</td>\n",
              "      <td>0.057408</td>\n",
              "      <td>-0.303010</td>\n",
              "      <td>0.271360</td>\n",
              "      <td>0.303070</td>\n",
              "      <td>-0.041741</td>\n",
              "      <td>0.56293</td>\n",
              "      <td>0.062314</td>\n",
              "      <td>-1.6413</td>\n",
              "      <td>-0.23018</td>\n",
              "      <td>1.74480</td>\n",
              "      <td>-0.205090</td>\n",
              "      <td>0.225210</td>\n",
              "      <td>-0.180140</td>\n",
              "      <td>-0.129580</td>\n",
              "      <td>-0.431660</td>\n",
              "      <td>0.650840</td>\n",
              "      <td>-0.172870</td>\n",
              "      <td>0.097956</td>\n",
              "      <td>0.395640</td>\n",
              "      <td>0.52575</td>\n",
              "      <td>0.394430</td>\n",
              "      <td>-0.444890</td>\n",
              "      <td>0.103510</td>\n",
              "      <td>0.228380</td>\n",
              "      <td>0.196760</td>\n",
              "      <td>0.163680</td>\n",
              "      <td>0.733660</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>-0.008545</td>\n",
              "      <td>-0.938440</td>\n",
              "      <td>0.60734</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                1         2         3    ...       298       299      300\n",
              "according -0.279610  0.137320  0.043514  ...  0.390080 -0.324610 -0.01826\n",
              "several   -0.278830  0.298990 -0.092609  ... -0.224160 -0.366290 -0.21762\n",
              "court      0.294920 -0.274790 -0.616680  ... -0.207100  0.015589  0.17391\n",
              "say        0.056090  0.200350 -0.136570  ... -0.501320 -0.021014  0.44665\n",
              "around    -0.469570  0.000895 -0.030859  ...  0.184050 -0.110230 -0.31083\n",
              "foreign   -0.095487 -0.209960 -0.199050  ...  0.043425 -0.902620  0.49778\n",
              "10        -0.441890  0.242480 -0.229970  ... -0.776380 -0.054478 -0.13019\n",
              "until      0.045036 -0.162020  0.401750  ... -0.506100  0.070201  0.31148\n",
              "set       -0.330680  0.174590 -0.325450  ... -0.400650  0.052764  0.54662\n",
              "political  0.561610 -0.114750  0.421820  ... -0.008545 -0.938440  0.60734\n",
              "\n",
              "[10 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htf4Jj_GVGMn"
      },
      "source": [
        "#### For later processing¶\n",
        "- 'sos' token to be at index = 0\n",
        "- 'eos' token to be at index = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCwWH7wUVja1"
      },
      "source": [
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeX20qB3V5Mu"
      },
      "source": [
        "#### Swapping 'sos' token index and 'eos' token index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwRGHmfXV7Hz"
      },
      "source": [
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ImKSH4WvOj"
      },
      "source": [
        "#### Creating Sorted Instance of word2idx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPWum1oWgVk"
      },
      "source": [
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAY3toaqDxXA"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
        "\n",
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` which will be used to replace rare words later.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b71WvS0-X7Xu"
      },
      "source": [
        "#### Creating 2 Seperate Classes for Input Lang and Ouput Lang\n",
        "\n",
        "Class 1 - > InputLang - built from Glove\n",
        "\n",
        "Class 2 - > OutputLang - built from Target Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-xmITKdYD90"
      },
      "source": [
        "class InputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "        self.word2count = { word : 1 for word in words }\n",
        "        self.index2word = { i : word for word, i in word2idx.items() }\n",
        "        self.n_words = 400001\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ngtfyUqa-tm"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class OutputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLdDKLAODduG"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFrQoWJbEOYd"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696X0f6eEMa9"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVorNRFCETgM"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD5V5PPHERbk"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = InputLang(lang2)\n",
        "        output_lang = OutputLang(lang1)\n",
        "    else:\n",
        "        input_lang = InputLang(lang1)\n",
        "        output_lang = OutputLang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjaZPWj-Ed1p"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpLh4takEcl9"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQpC4FE3EsXQ"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQl0eg8BH9BS",
        "outputId": "f521b883-43f6-41ad-f67a-adc4140edd4e"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400005\n",
            "fra 4345\n",
            "['i m working for mcdonald s .', 'je travaille chez mcdonald s .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Zbiow8b0rZ"
      },
      "source": [
        "#### Initializing weight Matrix\n",
        "\n",
        "We must build a matrix of weights that will be loaded into the PyTorch embedding layer. Its shape will be equal to:\n",
        "\n",
        "**(dataset’s vocabulary length, word vectors dimension)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC6WzLQScAG5"
      },
      "source": [
        "matrix_len = input_lang.n_words\n",
        "glove_dim = 300\n",
        "\n",
        "weights_matrix = np.zeros((matrix_len, glove_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(glove_dim, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IszO9MwEhLRe",
        "outputId": "0d5bdf39-dd70-45f7-df3b-fb6c68f5eec3"
      },
      "source": [
        "weights_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400005, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sklKeEAZEw9A"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJZ7qgeoGH25"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtVkjGRfe-Pb"
      },
      "source": [
        "# def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "#     num_embeddings, embedding_dim = weights_matrix.size()\n",
        "#     emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "#     emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "#     if non_trainable:\n",
        "#         emb_layer.weight.requires_grad = False\n",
        "\n",
        "#     return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUFStkIuEqo7"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, weights_matrix=None):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        if weights_matrix is None:\n",
        "          self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WzcqRwLGR_c"
      },
      "source": [
        "#Simple Decoder\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTpja5ExGQR_"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMxRg_-Ga5T"
      },
      "source": [
        "# Attention Decoder\n",
        "\n",
        "If only the context vector is passed between the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        "![image](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTgpqalYGaOR"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xa7siQKGqQE"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mj3FhJBGoS_"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rtejHTjG5Ia"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained network is exploited, it may exhibit instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWVqaIu-Gt3b"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQi5ocOHDuv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A3z6eQtG3rv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po1iCyBTHHIG"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gf1aSD1HFx_"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxLPtvy-HJv8"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kErPEK3PHIjD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BWg0QVLHMnS"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjgrd4yHLMS"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDyINR3HP8X"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41NOzzmRHOfs"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I3YLTtFHSkq"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bseo9ZViG_Go",
        "outputId": "5c995530-1de9-49d8-c632-3309cd71fa71"
      },
      "source": [
        "hidden_size = 300 #256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, weights_matrix).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6m 27s (- 90m 21s) (5000 6%) 3.4356\n",
            "12m 50s (- 83m 30s) (10000 13%) 2.7440\n",
            "19m 16s (- 77m 6s) (15000 20%) 2.3890\n",
            "25m 41s (- 70m 38s) (20000 26%) 2.1120\n",
            "32m 6s (- 64m 13s) (25000 33%) 1.8726\n",
            "38m 32s (- 57m 48s) (30000 40%) 1.6580\n",
            "44m 57s (- 51m 22s) (35000 46%) 1.5425\n",
            "51m 24s (- 44m 59s) (40000 53%) 1.3826\n",
            "57m 49s (- 38m 33s) (45000 60%) 1.2583\n",
            "64m 14s (- 32m 7s) (50000 66%) 1.1682\n",
            "70m 39s (- 25m 41s) (55000 73%) 1.0912\n",
            "77m 4s (- 19m 16s) (60000 80%) 1.0004\n",
            "83m 29s (- 12m 50s) (65000 86%) 0.9606\n",
            "89m 52s (- 6m 25s) (70000 93%) 0.9084\n",
            "96m 12s (- 0m 0s) (75000 100%) 0.8534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_rlcrR1Iqld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3e6547-e2ad-4a52-d3c0-ea544e37b148"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> she is being blackmailed by him .\n",
            "= il exerce sur elle du chantage .\n",
            "< elle va beaucoup lui lui . <EOS>\n",
            "\n",
            "> you re very religious aren t you ?\n",
            "= vous etes tres religieuses n est ce pas ?\n",
            "< vous etes tres religieux n est ce pas ? <EOS>\n",
            "\n",
            "> he is on the team .\n",
            "= il fait partie de l equipe .\n",
            "< il est partie de la equipe . <EOS>\n",
            "\n",
            "> you re the leader .\n",
            "= c est vous la chef .\n",
            "< vous etes la chef chef . <EOS>\n",
            "\n",
            "> you are too young to travel alone .\n",
            "= vous etes trop jeunes pour voyager seuls .\n",
            "< vous etes trop jeune pour voyager seul . <EOS>\n",
            "\n",
            "> you aren t supposed to swim here .\n",
            "= tu n es pas cense nager ici .\n",
            "< vous n etes pas censees nager ici . <EOS>\n",
            "\n",
            "> he s no saint .\n",
            "= il n est pas un saint .\n",
            "< ce n est pas un saint . <EOS>\n",
            "\n",
            "> you re a woman now .\n",
            "= vous etes desormais une femme .\n",
            "< vous etes une femme maintenant . <EOS>\n",
            "\n",
            "> we re surprised .\n",
            "= nous sommes surprises .\n",
            "< nous sommes surpris . <EOS>\n",
            "\n",
            "> we re open tomorrow .\n",
            "= nous sommes ouverts demain .\n",
            "< nous sommes ouverts demain . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4J1zvrwInJk"
      },
      "source": [
        "Visualizing Attention\n",
        "---------------------\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "1t-jIrj1HVL9",
        "outputId": "704b7bf6-f06e-4528-c828-f213b7343f4b"
      },
      "source": [
        "%matplotlib inline\n",
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7fb819dc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAECCAYAAABZiRbtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAK4ElEQVR4nO3dX4jld3nH8c/TmTVxtdRWc9Fkl24uxBKEJmUItoFexJbEKnqbgF6Uwt7UNhZBtHe9L2IvpLDEtAWDocRcSEi7lRopQhvdJFtrsiohteZfSWqx/oHmn08vZkpSicyZdp45c36+XrAwZ+Zw+PDNMu/8zjkzW90dAGDGz6x7AAAsmdACwCChBYBBQgsAg4QWAAYJLQAM2ojQVtXNVfWNqnqsqj667j1LVFWnq+r+qnq0qh6pqtvWvWmpqmqrqh6uqnvXvWWJqupNVXV3VX29qi5V1a+te9MSVdUf7n2v+FpVfaaqLl/3puPq2Ie2qraSfDLJu5Jck+TWqrpmvasW6aUkH+7ua5K8I8nvOecxtyW5tO4RC/anSf6mu385ya/EWR+6qroqyR8k2enutyfZSnLLelcdX8c+tEmuT/JYdz/e3S8kuSvJ+9a8aXG6+5nufmjv4+9n95vTVetdtTxVdSrJu5Pcvu4tS1RVP5fkN5J8Kkm6+4Xu/u56Vy3WdpLXV9V2kpNJnl7znmNrE0J7VZInXnX7yQjAqKo6k+S6JA+sd8kifSLJR5L8aN1DFurqJM8l+fO9p+dvr6o3rHvU0nT3U0n+JMm3kzyT5D+7+2/Xu+r42oTQcoSq6o1JPpvkQ939vXXvWZKqek+SZ7v7wXVvWbDtJL+a5M+6+7okP0zifR2HrKp+PrvPLF6d5Mokb6iq96931fG1CaF9KsnpV90+tfc5DllVnchuZO/s7nvWvWeBbkjy3qr6VnZfArmxqj693kmL82SSJ7v7f56NuTu74eVw/WaSf+nu57r7xST3JPn1NW86tjYhtF9J8taqurqqXpfdF9w/t+ZNi1NVld3XtS5198fXvWeJuvtj3X2qu89k9+/xF7rbVcAh6u5/S/JEVb1t71PvTPLoGict1beTvKOqTu5973hnvOnsJ9pe94D9dPdLVfXBJOez+862O7r7kTXPWqIbknwgyT9X1cW9z/1Rd9+3xk3wf/H7Se7c+x/zx5P8zpr3LE53P1BVdyd5KLs/sfBwknPrXXV8lX8mDwDmbMJTxwCwsYQWAAYJLQAMEloAGCS0ADBoo0JbVWfXvWHpnPE8Z3w0nPM8Z7yajQptEv9R5znjec74aDjnec54BZsWWgDYKCO/sOItv7DVZ06fOPTHfe47L+eKN28d+uMmyTe/enLkcTfNi3k+J3LZumcsmjM+Gs55njN+xX/lh3mhn6/X+trIr2A8c/pEvnz+9P53PEZuuvLadU8AYEM90H/3E7/mqWMAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGLRSaKvq5qr6RlU9VlUfnR4FAEuxb2iraivJJ5O8K8k1SW6tqmumhwHAEqxyRXt9kse6+/HufiHJXUneNzsLAJZhldBeleSJV91+cu9z/0tVna2qC1V14bnvvHxY+wBgox3am6G6+1x373T3zhVv3jqshwWAjbZKaJ9KcvpVt0/tfQ4A2Mcqof1KkrdW1dVV9boktyT53OwsAFiG7f3u0N0vVdUHk5xPspXkju5+ZHwZACzAvqFNku6+L8l9w1sAYHH8ZigAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMWukffj+ob371ZG668tqJh2aDnX/64ronHJi/x8D/lytaABgktAAwSGgBYJDQAsAgoQWAQUILAIOEFgAGCS0ADBJaABgktAAwSGgBYJDQAsAgoQWAQUILAIOEFgAGCS0ADBJaABgktAAwaN/QVtUdVfVsVX3tKAYBwJKsckX7F0luHt4BAIu0b2i7+++T/McRbAGAxfEaLQAM2j6sB6qqs0nOJsnlOXlYDwsAG+3Qrmi7+1x373T3zolcdlgPCwAbzVPHADBolR/v+UySf0jytqp6sqp+d34WACzDvq/RdvetRzEEAJbIU8cAMEhoAWCQ0ALAIKEFgEFCCwCDhBYABgktAAwSWgAYJLQAMEhoAWCQ0ALAIKEFgEFCCwCDhBYABgktAAwSWgAYJLQAMGh73QP46XHTldeue8KBnX/64ronHMgmnjEsnStaABgktAAwSGgBYJDQAsAgoQWAQUILAIOEFgAGCS0ADBJaABgktAAwSGgBYJDQAsAgoQWAQUILAIOEFgAGCS0ADBJaABgktAAwaN/QVtXpqrq/qh6tqkeq6rajGAYAS7C9wn1eSvLh7n6oqn42yYNV9fnufnR4GwBsvH2vaLv7me5+aO/j7ye5lOSq6WEAsAQHeo22qs4kuS7JAxNjAGBpVnnqOElSVW9M8tkkH+ru773G188mOZskl+fkoQ0EgE220hVtVZ3IbmTv7O57Xus+3X2uu3e6e+dELjvMjQCwsVZ513El+VSSS9398flJALAcq1zR3pDkA0lurKqLe39+e3gXACzCvq/RdveXktQRbAGAxfGboQBgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBB+4a2qi6vqi9X1T9V1SNV9cdHMQwAlmB7hfs8n+TG7v5BVZ1I8qWq+uvu/sfhbQCw8fYNbXd3kh/s3Tyx96cnRwHAUqz0Gm1VbVXVxSTPJvl8dz8wOwsAlmGl0Hb3y919bZJTSa6vqrf/+H2q6mxVXaiqCy/m+cPeCQAb6UDvOu7u7ya5P8nNr/G1c9290907J3LZYe0DgI22yruOr6iqN+19/Pokv5Xk69PDAGAJVnnX8S8m+cuq2spumP+qu++dnQUAy7DKu46/muS6I9gCAIvjN0MBwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwKDtdQ+A4+ymK69d94QDOf/0xXVPOLBNO2M4KFe0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFgkNACwCChBYBBQgsAg4QWAAYJLQAMEloAGCS0ADBIaAFg0Mqhraqtqnq4qu6dHAQAS3KQK9rbklyaGgIAS7RSaKvqVJJ3J7l9dg4ALMuqV7SfSPKRJD8a3AIAi7NvaKvqPUme7e4H97nf2aq6UFUXXszzhzYQADbZKle0NyR5b1V9K8ldSW6sqk//+J26+1x373T3zolcdsgzAWAz7Rva7v5Yd5/q7jNJbknyhe5+//gyAFgAP0cLAIO2D3Ln7v5iki+OLAGABXJFCwCDhBYABgktAAwSWgAYJLQAMEhoAWCQ0ALAIKEFgEFCCwCDhBYABgktAAwSWgAYJLQAMEhoAWCQ0ALAIKEFgEFCCwCDqrsP/0Grnkvyr4f+wMlbkvz7wOPyCmc8zxkfDec8zxm/4pe6+4rX+sJIaKdU1YXu3ln3jiVzxvOc8dFwzvOc8Wo8dQwAg4QWAAZtWmjPrXvATwFnPM8ZHw3nPM8Zr2CjXqMFgE2zaVe0ALBRhBYABgktAAwSWgAYJLQAMOi/AfVTBoMbKrZDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zYmdIDIvbR"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6iRp2UgrIxPQ",
        "outputId": "0fb34969-dd90-4d97-d56f-0e8b8f2d51a5"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"he is painting a picture .\")\n",
        "\n",
        "evaluateAndShowAttention(\"why not try that delicious wine ?\")\n",
        "\n",
        "evaluateAndShowAttention(\"she is not a poet but a novelist .\")\n",
        "\n",
        "evaluateAndShowAttention(\"you re too skinny .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = he is painting a picture .\n",
            "output = il est en train de faire un roman . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = why not try that delicious wine ?\n",
            "output = ce n est pas pas vrai ce pas ? <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = she is not a poet but a novelist .\n",
            "output = elle n est pas poete mais romanciere . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = you re too skinny .\n",
            "output = vous etes trop maigrichonne . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}