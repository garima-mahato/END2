{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END2_Session10_END2 Translation using Seq2Seq and Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxCla6HFCtRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea3f1b6-9df9-448a-9efb-bfec33eef7d9"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install bcolz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 15.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 10.8MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 358kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 368kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 378kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 389kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 399kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 419kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 430kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 440kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 450kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 460kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 471kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 706kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 716kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 737kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 747kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 757kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 768kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 778kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 788kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 798kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 808kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 829kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 839kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 849kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 860kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 870kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 880kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 890kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 901kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 921kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 931kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 942kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 952kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2650833 sha256=f413d5199b0bd3f3ef78fffddcb392f0c3791905ccce0cc9adedf2b2b7d8b2da\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOr51ZdbymLp",
        "outputId": "5c3f69da-7365-43de-f630-5d0e8b51a389"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Sm1RVyDOVE"
      },
      "source": [
        "\n",
        "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "*******************************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "[PyTorch Source](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
        "write our own classes and functions to preprocess the data to do our NLP\n",
        "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
        "learn how `torchtext` can handle much of this preprocessing for you in the\n",
        "three tutorials immediately following this one.\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  [Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html): A 60 Minute Blitz to get started with PyTorch in general\n",
        "-  [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) for a wide and deep overview\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "-  [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "-  [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)\n",
        "\n",
        "\n",
        "\n",
        "![Image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EEWOCJLDD-E"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import bcolz  # to process the data from Glove File \n",
        "import pickle # to dump and load pretrained glove vectors \n",
        "import copy   # to make deepcopy of python lists and dictionaries\n",
        "import operator\n",
        "import numpy as np\n",
        "from pandas import DataFrame # to visualize the glove word embeddings in form of DataFrame\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30yEbATJDXCF"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "[This question on Open Data Stack\n",
        "Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)\n",
        "pointed me/him to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_EPl_QFDUSE",
        "outputId": "d788b5e8-be44-4163-e74e-504be2c6b15c"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 17:39:48--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.86.37.74, 99.86.37.53, 99.86.37.116, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.86.37.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-07-17 17:39:48 (48.5 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugKjISLqznF-"
      },
      "source": [
        "Download  and Unzip Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVn2PBI6zdRg",
        "outputId": "51de98a7-0622-4299-d2c6-829ceef63024"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 17:47:40--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-07-17 17:47:40--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-07-17 17:47:40--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 40s  \n",
            "\n",
            "2021-07-17 17:50:20 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Opt0fXOfRc"
      },
      "source": [
        "print(len(lines))          # number of words (aka vocabulary size)\n",
        "print(len(lines[0]))       # length of a line\n",
        "print(lines[130][0])       # word 130\n",
        "print(lines[130][1:])      # vector representation of word 130\n",
        "print(len(lines[130][1:])) # dimensionality of word 130\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ4g0l2aRYAr"
      },
      "source": [
        "#### Loading and Saving Glove Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkdS9ZtC1oXB"
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir=f'/gdrive/MyDrive/TSAI_END2/Session10/6B.300.dat', mode='w')\n",
        "\n",
        "with open(f'/content/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "    \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400001, 300)), rootdir=f'/gdrive/MyDrive/TSAI_END2/Session10/6B.300.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'/content/drive/MyDrive/TSAI_END2/Session10/glove.6B.300_idx.pkl', 'wb'))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdPjq-hd8UYW"
      },
      "source": [
        "pickle.dump(words, open(f'./glove.6B.300_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'./glove.6B.300_idx.pkl', 'wb'))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpeptdfO9tro",
        "outputId": "21418792-b162-47e1-de1d-db515d55057d"
      },
      "source": [
        "!ls '/content/drive/MyDrive/TSAI_END2/Session10'"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6B.300.dat  glove.6B.300_idx.pkl    glove.6B.50_idx.pkl\n",
            "6B.50.dat   glove.6B.300_words.pkl  glove.6B.50_words.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIKUn1lcu2a"
      },
      "source": [
        "#### Load saved vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UilzgUKWRfWZ"
      },
      "source": [
        "vectors = bcolz.open(f'/gdrive/MyDrive/TSAI_END2/Session10/6B.300.dat')[:]\n",
        "words = pickle.load(open(f'/gdrive/MyDrive/TSAI_END2/Session10/glove.6B.300_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open(f'/gdrive/MyDrive/TSAI_END2/Session10/glove.6B.300_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCBGOD0XT2bP"
      },
      "source": [
        "glove_df = DataFrame(vectors, columns=range(1,301), index=words)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyUmAVY4Uxrt"
      },
      "source": [
        "#### Visualize the Glove Word Embeddings by making a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "B259PP29T8Ro",
        "outputId": "9b562a98-e75c-4ada-bfba-269e7a9fc0a8"
      },
      "source": [
        "glove_df[200:210]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>according</th>\n",
              "      <td>-0.279610</td>\n",
              "      <td>0.137320</td>\n",
              "      <td>0.043514</td>\n",
              "      <td>0.333020</td>\n",
              "      <td>-0.168460</td>\n",
              "      <td>0.067218</td>\n",
              "      <td>-0.166420</td>\n",
              "      <td>0.157180</td>\n",
              "      <td>-0.121350</td>\n",
              "      <td>-1.73860</td>\n",
              "      <td>-0.024858</td>\n",
              "      <td>-0.265710</td>\n",
              "      <td>0.175370</td>\n",
              "      <td>0.173250</td>\n",
              "      <td>-0.002423</td>\n",
              "      <td>0.159280</td>\n",
              "      <td>-0.186030</td>\n",
              "      <td>0.251630</td>\n",
              "      <td>-0.386520</td>\n",
              "      <td>-0.336200</td>\n",
              "      <td>0.126930</td>\n",
              "      <td>0.073719</td>\n",
              "      <td>0.249740</td>\n",
              "      <td>0.456310</td>\n",
              "      <td>-0.20157</td>\n",
              "      <td>0.017940</td>\n",
              "      <td>-0.085641</td>\n",
              "      <td>0.082276</td>\n",
              "      <td>0.202490</td>\n",
              "      <td>-0.137970</td>\n",
              "      <td>0.040790</td>\n",
              "      <td>0.547840</td>\n",
              "      <td>-0.041509</td>\n",
              "      <td>0.19313</td>\n",
              "      <td>-0.80545</td>\n",
              "      <td>-0.226530</td>\n",
              "      <td>0.200270</td>\n",
              "      <td>-0.039198</td>\n",
              "      <td>-0.175200</td>\n",
              "      <td>-0.179190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063811</td>\n",
              "      <td>0.47207</td>\n",
              "      <td>0.037923</td>\n",
              "      <td>-0.361250</td>\n",
              "      <td>0.209150</td>\n",
              "      <td>-0.245690</td>\n",
              "      <td>0.258970</td>\n",
              "      <td>-0.060094</td>\n",
              "      <td>-0.137910</td>\n",
              "      <td>-0.233440</td>\n",
              "      <td>-0.192400</td>\n",
              "      <td>-0.188730</td>\n",
              "      <td>0.157660</td>\n",
              "      <td>0.178660</td>\n",
              "      <td>0.11679</td>\n",
              "      <td>-0.192830</td>\n",
              "      <td>-1.9656</td>\n",
              "      <td>-0.10005</td>\n",
              "      <td>0.48721</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>0.010740</td>\n",
              "      <td>-0.108050</td>\n",
              "      <td>0.054460</td>\n",
              "      <td>-0.356230</td>\n",
              "      <td>0.142410</td>\n",
              "      <td>0.184360</td>\n",
              "      <td>-0.032794</td>\n",
              "      <td>-0.066567</td>\n",
              "      <td>-0.16291</td>\n",
              "      <td>0.345220</td>\n",
              "      <td>-0.013469</td>\n",
              "      <td>-0.219000</td>\n",
              "      <td>0.037849</td>\n",
              "      <td>0.155860</td>\n",
              "      <td>0.442500</td>\n",
              "      <td>0.433770</td>\n",
              "      <td>-0.192290</td>\n",
              "      <td>0.390080</td>\n",
              "      <td>-0.324610</td>\n",
              "      <td>-0.01826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>several</th>\n",
              "      <td>-0.278830</td>\n",
              "      <td>0.298990</td>\n",
              "      <td>-0.092609</td>\n",
              "      <td>-0.149200</td>\n",
              "      <td>0.102150</td>\n",
              "      <td>-0.187240</td>\n",
              "      <td>-0.057946</td>\n",
              "      <td>0.050455</td>\n",
              "      <td>-0.045006</td>\n",
              "      <td>-1.36610</td>\n",
              "      <td>-0.197030</td>\n",
              "      <td>0.145630</td>\n",
              "      <td>-0.187190</td>\n",
              "      <td>0.109940</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>-0.108720</td>\n",
              "      <td>-0.462160</td>\n",
              "      <td>0.170320</td>\n",
              "      <td>0.117040</td>\n",
              "      <td>-0.029810</td>\n",
              "      <td>-0.005754</td>\n",
              "      <td>-0.049542</td>\n",
              "      <td>0.319010</td>\n",
              "      <td>0.118340</td>\n",
              "      <td>-0.32802</td>\n",
              "      <td>-0.151250</td>\n",
              "      <td>-0.486250</td>\n",
              "      <td>0.179170</td>\n",
              "      <td>-0.274930</td>\n",
              "      <td>0.317760</td>\n",
              "      <td>0.235180</td>\n",
              "      <td>0.155060</td>\n",
              "      <td>0.100110</td>\n",
              "      <td>0.23292</td>\n",
              "      <td>-0.55807</td>\n",
              "      <td>-0.352460</td>\n",
              "      <td>0.081194</td>\n",
              "      <td>0.386270</td>\n",
              "      <td>-0.094630</td>\n",
              "      <td>-0.074292</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017501</td>\n",
              "      <td>0.34991</td>\n",
              "      <td>0.358650</td>\n",
              "      <td>-0.209420</td>\n",
              "      <td>0.204730</td>\n",
              "      <td>-0.258020</td>\n",
              "      <td>-0.770900</td>\n",
              "      <td>-0.010413</td>\n",
              "      <td>0.049296</td>\n",
              "      <td>0.006965</td>\n",
              "      <td>0.144020</td>\n",
              "      <td>0.279530</td>\n",
              "      <td>-0.190090</td>\n",
              "      <td>0.130970</td>\n",
              "      <td>0.27707</td>\n",
              "      <td>0.009039</td>\n",
              "      <td>-2.6111</td>\n",
              "      <td>-0.17615</td>\n",
              "      <td>0.25067</td>\n",
              "      <td>0.108000</td>\n",
              "      <td>-0.391450</td>\n",
              "      <td>0.023849</td>\n",
              "      <td>0.234420</td>\n",
              "      <td>0.049562</td>\n",
              "      <td>-0.275360</td>\n",
              "      <td>0.289470</td>\n",
              "      <td>-0.143550</td>\n",
              "      <td>0.219270</td>\n",
              "      <td>0.46774</td>\n",
              "      <td>-0.174360</td>\n",
              "      <td>-0.064541</td>\n",
              "      <td>-0.324480</td>\n",
              "      <td>-0.180430</td>\n",
              "      <td>0.003164</td>\n",
              "      <td>-0.176020</td>\n",
              "      <td>0.478750</td>\n",
              "      <td>-0.237680</td>\n",
              "      <td>-0.224160</td>\n",
              "      <td>-0.366290</td>\n",
              "      <td>-0.21762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>court</th>\n",
              "      <td>0.294920</td>\n",
              "      <td>-0.274790</td>\n",
              "      <td>-0.616680</td>\n",
              "      <td>-0.055869</td>\n",
              "      <td>0.385780</td>\n",
              "      <td>-0.582630</td>\n",
              "      <td>0.730860</td>\n",
              "      <td>-0.119370</td>\n",
              "      <td>-0.041382</td>\n",
              "      <td>-1.42530</td>\n",
              "      <td>-0.305880</td>\n",
              "      <td>-0.056891</td>\n",
              "      <td>-0.195040</td>\n",
              "      <td>0.198430</td>\n",
              "      <td>-0.554840</td>\n",
              "      <td>0.283550</td>\n",
              "      <td>-0.537580</td>\n",
              "      <td>-0.305180</td>\n",
              "      <td>0.569070</td>\n",
              "      <td>-0.239610</td>\n",
              "      <td>0.186620</td>\n",
              "      <td>-0.184960</td>\n",
              "      <td>0.440740</td>\n",
              "      <td>-0.083826</td>\n",
              "      <td>-0.25820</td>\n",
              "      <td>-0.044409</td>\n",
              "      <td>0.139150</td>\n",
              "      <td>-0.541320</td>\n",
              "      <td>-0.460620</td>\n",
              "      <td>0.581210</td>\n",
              "      <td>-0.022778</td>\n",
              "      <td>0.173250</td>\n",
              "      <td>0.178510</td>\n",
              "      <td>0.29736</td>\n",
              "      <td>-0.82185</td>\n",
              "      <td>-0.389630</td>\n",
              "      <td>0.094130</td>\n",
              "      <td>-0.205660</td>\n",
              "      <td>-0.772460</td>\n",
              "      <td>0.181290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189250</td>\n",
              "      <td>-0.16735</td>\n",
              "      <td>0.412260</td>\n",
              "      <td>0.142210</td>\n",
              "      <td>0.170160</td>\n",
              "      <td>-0.354630</td>\n",
              "      <td>0.061339</td>\n",
              "      <td>0.343660</td>\n",
              "      <td>-0.046048</td>\n",
              "      <td>-1.083400</td>\n",
              "      <td>0.439800</td>\n",
              "      <td>-0.694310</td>\n",
              "      <td>0.207420</td>\n",
              "      <td>0.119580</td>\n",
              "      <td>0.19484</td>\n",
              "      <td>-0.004723</td>\n",
              "      <td>-2.1147</td>\n",
              "      <td>0.18858</td>\n",
              "      <td>1.13600</td>\n",
              "      <td>0.931650</td>\n",
              "      <td>0.113970</td>\n",
              "      <td>0.024541</td>\n",
              "      <td>0.015915</td>\n",
              "      <td>0.209480</td>\n",
              "      <td>-0.301930</td>\n",
              "      <td>-0.062889</td>\n",
              "      <td>0.281870</td>\n",
              "      <td>0.570090</td>\n",
              "      <td>-0.53056</td>\n",
              "      <td>0.285680</td>\n",
              "      <td>0.437920</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.241620</td>\n",
              "      <td>0.506370</td>\n",
              "      <td>0.290290</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.136190</td>\n",
              "      <td>-0.207100</td>\n",
              "      <td>0.015589</td>\n",
              "      <td>0.17391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>say</th>\n",
              "      <td>0.056090</td>\n",
              "      <td>0.200350</td>\n",
              "      <td>-0.136570</td>\n",
              "      <td>0.057160</td>\n",
              "      <td>-0.127080</td>\n",
              "      <td>-0.201790</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.261590</td>\n",
              "      <td>-0.052328</td>\n",
              "      <td>-1.75730</td>\n",
              "      <td>0.076761</td>\n",
              "      <td>-0.088249</td>\n",
              "      <td>-0.053148</td>\n",
              "      <td>0.581910</td>\n",
              "      <td>0.458550</td>\n",
              "      <td>0.270420</td>\n",
              "      <td>0.020048</td>\n",
              "      <td>0.052781</td>\n",
              "      <td>0.235690</td>\n",
              "      <td>0.287740</td>\n",
              "      <td>0.162650</td>\n",
              "      <td>0.162660</td>\n",
              "      <td>0.349910</td>\n",
              "      <td>-0.291750</td>\n",
              "      <td>-0.67895</td>\n",
              "      <td>0.058927</td>\n",
              "      <td>0.219020</td>\n",
              "      <td>-0.378070</td>\n",
              "      <td>-0.041854</td>\n",
              "      <td>-0.118250</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>0.572670</td>\n",
              "      <td>-0.604510</td>\n",
              "      <td>-0.14544</td>\n",
              "      <td>-0.63381</td>\n",
              "      <td>0.081092</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.210160</td>\n",
              "      <td>0.180770</td>\n",
              "      <td>-0.514370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.145000</td>\n",
              "      <td>-0.19456</td>\n",
              "      <td>-0.159280</td>\n",
              "      <td>0.204040</td>\n",
              "      <td>-0.080843</td>\n",
              "      <td>0.223510</td>\n",
              "      <td>-0.135650</td>\n",
              "      <td>-0.096678</td>\n",
              "      <td>-0.085022</td>\n",
              "      <td>0.077191</td>\n",
              "      <td>0.182990</td>\n",
              "      <td>0.298280</td>\n",
              "      <td>0.107660</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>0.36590</td>\n",
              "      <td>0.197160</td>\n",
              "      <td>-2.4170</td>\n",
              "      <td>0.10348</td>\n",
              "      <td>0.37408</td>\n",
              "      <td>-0.036688</td>\n",
              "      <td>0.004247</td>\n",
              "      <td>0.044540</td>\n",
              "      <td>-0.124100</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.063308</td>\n",
              "      <td>0.182960</td>\n",
              "      <td>0.269000</td>\n",
              "      <td>-0.223500</td>\n",
              "      <td>-0.17115</td>\n",
              "      <td>0.082492</td>\n",
              "      <td>0.189930</td>\n",
              "      <td>-0.544910</td>\n",
              "      <td>0.171560</td>\n",
              "      <td>-0.017652</td>\n",
              "      <td>0.174040</td>\n",
              "      <td>-0.603500</td>\n",
              "      <td>-0.107820</td>\n",
              "      <td>-0.501320</td>\n",
              "      <td>-0.021014</td>\n",
              "      <td>0.44665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>around</th>\n",
              "      <td>-0.469570</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>-0.030859</td>\n",
              "      <td>-0.107720</td>\n",
              "      <td>-0.025968</td>\n",
              "      <td>0.408180</td>\n",
              "      <td>0.218770</td>\n",
              "      <td>0.382240</td>\n",
              "      <td>0.105420</td>\n",
              "      <td>-1.64180</td>\n",
              "      <td>-0.113610</td>\n",
              "      <td>0.291120</td>\n",
              "      <td>0.057045</td>\n",
              "      <td>0.199720</td>\n",
              "      <td>0.291610</td>\n",
              "      <td>0.366450</td>\n",
              "      <td>-0.456570</td>\n",
              "      <td>0.454240</td>\n",
              "      <td>-0.093902</td>\n",
              "      <td>0.331720</td>\n",
              "      <td>0.299060</td>\n",
              "      <td>0.338220</td>\n",
              "      <td>0.281110</td>\n",
              "      <td>-0.142310</td>\n",
              "      <td>0.18127</td>\n",
              "      <td>-0.004856</td>\n",
              "      <td>-0.112540</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>-0.254640</td>\n",
              "      <td>0.506790</td>\n",
              "      <td>0.098070</td>\n",
              "      <td>0.163230</td>\n",
              "      <td>-0.481590</td>\n",
              "      <td>0.55863</td>\n",
              "      <td>-0.67272</td>\n",
              "      <td>-0.025186</td>\n",
              "      <td>0.163960</td>\n",
              "      <td>0.465680</td>\n",
              "      <td>-0.088058</td>\n",
              "      <td>-0.114930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850180</td>\n",
              "      <td>0.27484</td>\n",
              "      <td>0.119640</td>\n",
              "      <td>-0.116100</td>\n",
              "      <td>0.069383</td>\n",
              "      <td>-0.413810</td>\n",
              "      <td>-0.262600</td>\n",
              "      <td>-0.124260</td>\n",
              "      <td>0.209350</td>\n",
              "      <td>-0.061349</td>\n",
              "      <td>-0.152320</td>\n",
              "      <td>0.141830</td>\n",
              "      <td>0.320900</td>\n",
              "      <td>0.116200</td>\n",
              "      <td>0.21903</td>\n",
              "      <td>-0.179330</td>\n",
              "      <td>-2.1543</td>\n",
              "      <td>-0.41776</td>\n",
              "      <td>0.08200</td>\n",
              "      <td>0.206420</td>\n",
              "      <td>-0.344030</td>\n",
              "      <td>-0.122760</td>\n",
              "      <td>-0.075667</td>\n",
              "      <td>-0.065374</td>\n",
              "      <td>-0.269220</td>\n",
              "      <td>0.390050</td>\n",
              "      <td>0.199480</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.22481</td>\n",
              "      <td>-0.043952</td>\n",
              "      <td>0.573730</td>\n",
              "      <td>-0.227680</td>\n",
              "      <td>-0.388810</td>\n",
              "      <td>0.240210</td>\n",
              "      <td>-0.081779</td>\n",
              "      <td>0.140490</td>\n",
              "      <td>0.036649</td>\n",
              "      <td>0.184050</td>\n",
              "      <td>-0.110230</td>\n",
              "      <td>-0.31083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>foreign</th>\n",
              "      <td>-0.095487</td>\n",
              "      <td>-0.209960</td>\n",
              "      <td>-0.199050</td>\n",
              "      <td>0.417170</td>\n",
              "      <td>-0.034925</td>\n",
              "      <td>-0.194490</td>\n",
              "      <td>-0.381970</td>\n",
              "      <td>0.391020</td>\n",
              "      <td>-0.281870</td>\n",
              "      <td>-2.79180</td>\n",
              "      <td>0.354780</td>\n",
              "      <td>0.162590</td>\n",
              "      <td>-0.333660</td>\n",
              "      <td>0.025810</td>\n",
              "      <td>-0.335900</td>\n",
              "      <td>-0.808810</td>\n",
              "      <td>0.071127</td>\n",
              "      <td>0.015254</td>\n",
              "      <td>-0.131030</td>\n",
              "      <td>-0.122410</td>\n",
              "      <td>-0.198780</td>\n",
              "      <td>0.070378</td>\n",
              "      <td>0.983250</td>\n",
              "      <td>-0.055934</td>\n",
              "      <td>-0.30992</td>\n",
              "      <td>0.323390</td>\n",
              "      <td>0.192530</td>\n",
              "      <td>0.245240</td>\n",
              "      <td>-0.130560</td>\n",
              "      <td>0.260320</td>\n",
              "      <td>-0.476990</td>\n",
              "      <td>-0.203560</td>\n",
              "      <td>0.213620</td>\n",
              "      <td>0.04692</td>\n",
              "      <td>-0.89056</td>\n",
              "      <td>-0.688440</td>\n",
              "      <td>0.192790</td>\n",
              "      <td>-0.446870</td>\n",
              "      <td>0.116330</td>\n",
              "      <td>0.271760</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.226100</td>\n",
              "      <td>-0.40134</td>\n",
              "      <td>0.082960</td>\n",
              "      <td>0.521950</td>\n",
              "      <td>0.833800</td>\n",
              "      <td>-0.722610</td>\n",
              "      <td>-0.072281</td>\n",
              "      <td>-0.001010</td>\n",
              "      <td>-0.394940</td>\n",
              "      <td>0.068606</td>\n",
              "      <td>-0.099127</td>\n",
              "      <td>0.304630</td>\n",
              "      <td>-0.454250</td>\n",
              "      <td>-0.055678</td>\n",
              "      <td>-0.37260</td>\n",
              "      <td>0.151170</td>\n",
              "      <td>-1.6924</td>\n",
              "      <td>-0.41482</td>\n",
              "      <td>0.40577</td>\n",
              "      <td>-0.217920</td>\n",
              "      <td>0.402590</td>\n",
              "      <td>-0.233630</td>\n",
              "      <td>0.250910</td>\n",
              "      <td>-0.123690</td>\n",
              "      <td>0.548910</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>0.305510</td>\n",
              "      <td>0.32427</td>\n",
              "      <td>0.134810</td>\n",
              "      <td>-0.037153</td>\n",
              "      <td>-0.541010</td>\n",
              "      <td>0.109550</td>\n",
              "      <td>-0.251420</td>\n",
              "      <td>0.260630</td>\n",
              "      <td>0.715130</td>\n",
              "      <td>-0.140730</td>\n",
              "      <td>0.043425</td>\n",
              "      <td>-0.902620</td>\n",
              "      <td>0.49778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.441890</td>\n",
              "      <td>0.242480</td>\n",
              "      <td>-0.229970</td>\n",
              "      <td>0.172260</td>\n",
              "      <td>-0.096344</td>\n",
              "      <td>0.136520</td>\n",
              "      <td>0.039942</td>\n",
              "      <td>0.120760</td>\n",
              "      <td>0.389680</td>\n",
              "      <td>-0.96754</td>\n",
              "      <td>0.291700</td>\n",
              "      <td>0.275010</td>\n",
              "      <td>-0.175220</td>\n",
              "      <td>0.428170</td>\n",
              "      <td>0.012163</td>\n",
              "      <td>-0.045286</td>\n",
              "      <td>-0.129230</td>\n",
              "      <td>-0.029375</td>\n",
              "      <td>0.102550</td>\n",
              "      <td>-0.350450</td>\n",
              "      <td>-0.212720</td>\n",
              "      <td>-0.130870</td>\n",
              "      <td>-0.012698</td>\n",
              "      <td>0.092539</td>\n",
              "      <td>-0.06211</td>\n",
              "      <td>0.190630</td>\n",
              "      <td>-0.344460</td>\n",
              "      <td>-0.141980</td>\n",
              "      <td>0.009539</td>\n",
              "      <td>-0.286660</td>\n",
              "      <td>-0.187720</td>\n",
              "      <td>0.611720</td>\n",
              "      <td>0.137120</td>\n",
              "      <td>0.17352</td>\n",
              "      <td>-1.03470</td>\n",
              "      <td>-0.230870</td>\n",
              "      <td>0.407430</td>\n",
              "      <td>0.433120</td>\n",
              "      <td>-0.360320</td>\n",
              "      <td>0.036961</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279180</td>\n",
              "      <td>0.04196</td>\n",
              "      <td>0.064526</td>\n",
              "      <td>-0.200040</td>\n",
              "      <td>0.295430</td>\n",
              "      <td>-0.187930</td>\n",
              "      <td>0.192390</td>\n",
              "      <td>-0.072376</td>\n",
              "      <td>0.274850</td>\n",
              "      <td>0.285530</td>\n",
              "      <td>-0.097114</td>\n",
              "      <td>0.004545</td>\n",
              "      <td>0.148490</td>\n",
              "      <td>0.090044</td>\n",
              "      <td>0.13226</td>\n",
              "      <td>0.028023</td>\n",
              "      <td>-1.9839</td>\n",
              "      <td>-0.42138</td>\n",
              "      <td>-0.21883</td>\n",
              "      <td>0.160240</td>\n",
              "      <td>0.032295</td>\n",
              "      <td>-0.058029</td>\n",
              "      <td>0.103180</td>\n",
              "      <td>0.123430</td>\n",
              "      <td>-0.168160</td>\n",
              "      <td>0.682780</td>\n",
              "      <td>-0.516350</td>\n",
              "      <td>-0.147090</td>\n",
              "      <td>0.12471</td>\n",
              "      <td>0.165170</td>\n",
              "      <td>0.070977</td>\n",
              "      <td>-0.389240</td>\n",
              "      <td>-0.269190</td>\n",
              "      <td>-0.342620</td>\n",
              "      <td>0.324960</td>\n",
              "      <td>0.279240</td>\n",
              "      <td>-0.266480</td>\n",
              "      <td>-0.776380</td>\n",
              "      <td>-0.054478</td>\n",
              "      <td>-0.13019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>until</th>\n",
              "      <td>0.045036</td>\n",
              "      <td>-0.162020</td>\n",
              "      <td>0.401750</td>\n",
              "      <td>-0.032719</td>\n",
              "      <td>0.157200</td>\n",
              "      <td>-0.201640</td>\n",
              "      <td>0.112720</td>\n",
              "      <td>-0.183880</td>\n",
              "      <td>0.233380</td>\n",
              "      <td>-1.86600</td>\n",
              "      <td>-0.240630</td>\n",
              "      <td>-0.107330</td>\n",
              "      <td>0.321970</td>\n",
              "      <td>-0.139860</td>\n",
              "      <td>-0.089932</td>\n",
              "      <td>0.090911</td>\n",
              "      <td>0.179980</td>\n",
              "      <td>-0.082335</td>\n",
              "      <td>0.108850</td>\n",
              "      <td>0.146420</td>\n",
              "      <td>0.186290</td>\n",
              "      <td>0.090300</td>\n",
              "      <td>0.239850</td>\n",
              "      <td>-0.092648</td>\n",
              "      <td>-0.22982</td>\n",
              "      <td>-0.096040</td>\n",
              "      <td>-0.224780</td>\n",
              "      <td>-0.145510</td>\n",
              "      <td>-0.636020</td>\n",
              "      <td>0.010182</td>\n",
              "      <td>0.195490</td>\n",
              "      <td>0.153900</td>\n",
              "      <td>0.271010</td>\n",
              "      <td>0.30660</td>\n",
              "      <td>-0.77808</td>\n",
              "      <td>-0.012936</td>\n",
              "      <td>-0.094754</td>\n",
              "      <td>0.559800</td>\n",
              "      <td>-0.091743</td>\n",
              "      <td>0.416900</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.444470</td>\n",
              "      <td>-0.34940</td>\n",
              "      <td>-0.176270</td>\n",
              "      <td>-0.164460</td>\n",
              "      <td>-0.250770</td>\n",
              "      <td>-0.295420</td>\n",
              "      <td>0.013784</td>\n",
              "      <td>-0.373670</td>\n",
              "      <td>0.083342</td>\n",
              "      <td>-0.074837</td>\n",
              "      <td>-0.108870</td>\n",
              "      <td>0.145720</td>\n",
              "      <td>-0.053223</td>\n",
              "      <td>0.225040</td>\n",
              "      <td>0.49595</td>\n",
              "      <td>0.014406</td>\n",
              "      <td>-2.0235</td>\n",
              "      <td>-0.39581</td>\n",
              "      <td>0.16585</td>\n",
              "      <td>0.170420</td>\n",
              "      <td>-0.327020</td>\n",
              "      <td>-0.268010</td>\n",
              "      <td>-0.005349</td>\n",
              "      <td>0.118980</td>\n",
              "      <td>0.195720</td>\n",
              "      <td>0.491120</td>\n",
              "      <td>0.041823</td>\n",
              "      <td>0.249410</td>\n",
              "      <td>-0.36564</td>\n",
              "      <td>-0.020910</td>\n",
              "      <td>-0.197260</td>\n",
              "      <td>-0.091085</td>\n",
              "      <td>0.112990</td>\n",
              "      <td>-0.275180</td>\n",
              "      <td>-0.317930</td>\n",
              "      <td>0.384110</td>\n",
              "      <td>-0.136790</td>\n",
              "      <td>-0.506100</td>\n",
              "      <td>0.070201</td>\n",
              "      <td>0.31148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>-0.330680</td>\n",
              "      <td>0.174590</td>\n",
              "      <td>-0.325450</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>0.125360</td>\n",
              "      <td>-0.074719</td>\n",
              "      <td>-0.046026</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>-0.045669</td>\n",
              "      <td>-1.33110</td>\n",
              "      <td>0.409900</td>\n",
              "      <td>0.419970</td>\n",
              "      <td>0.073290</td>\n",
              "      <td>-0.034584</td>\n",
              "      <td>-0.325820</td>\n",
              "      <td>0.022083</td>\n",
              "      <td>0.067834</td>\n",
              "      <td>-0.207580</td>\n",
              "      <td>-0.048283</td>\n",
              "      <td>0.091015</td>\n",
              "      <td>0.089587</td>\n",
              "      <td>-0.046688</td>\n",
              "      <td>0.185640</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.24787</td>\n",
              "      <td>0.058006</td>\n",
              "      <td>-0.097955</td>\n",
              "      <td>-0.160070</td>\n",
              "      <td>-0.260060</td>\n",
              "      <td>0.204210</td>\n",
              "      <td>0.096925</td>\n",
              "      <td>0.307040</td>\n",
              "      <td>-0.025169</td>\n",
              "      <td>0.62919</td>\n",
              "      <td>-0.98415</td>\n",
              "      <td>0.038619</td>\n",
              "      <td>-0.375850</td>\n",
              "      <td>0.301880</td>\n",
              "      <td>-0.263370</td>\n",
              "      <td>0.428480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.228250</td>\n",
              "      <td>-0.27699</td>\n",
              "      <td>0.250080</td>\n",
              "      <td>0.067117</td>\n",
              "      <td>-0.027048</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.335750</td>\n",
              "      <td>0.060869</td>\n",
              "      <td>0.519090</td>\n",
              "      <td>-0.048897</td>\n",
              "      <td>-0.067377</td>\n",
              "      <td>-0.064250</td>\n",
              "      <td>0.609530</td>\n",
              "      <td>0.639080</td>\n",
              "      <td>-0.14792</td>\n",
              "      <td>0.170250</td>\n",
              "      <td>-2.1690</td>\n",
              "      <td>0.17169</td>\n",
              "      <td>0.41858</td>\n",
              "      <td>-0.148040</td>\n",
              "      <td>0.350980</td>\n",
              "      <td>-0.218970</td>\n",
              "      <td>-0.295720</td>\n",
              "      <td>-0.060498</td>\n",
              "      <td>-0.574690</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>-0.441010</td>\n",
              "      <td>0.023605</td>\n",
              "      <td>-0.14858</td>\n",
              "      <td>0.029384</td>\n",
              "      <td>-0.089098</td>\n",
              "      <td>-0.192170</td>\n",
              "      <td>-0.269420</td>\n",
              "      <td>0.147910</td>\n",
              "      <td>0.145190</td>\n",
              "      <td>0.973710</td>\n",
              "      <td>-0.121230</td>\n",
              "      <td>-0.400650</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>0.54662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>political</th>\n",
              "      <td>0.561610</td>\n",
              "      <td>-0.114750</td>\n",
              "      <td>0.421820</td>\n",
              "      <td>-0.146580</td>\n",
              "      <td>0.559030</td>\n",
              "      <td>-0.055492</td>\n",
              "      <td>-0.047198</td>\n",
              "      <td>-0.180620</td>\n",
              "      <td>0.451410</td>\n",
              "      <td>-2.19460</td>\n",
              "      <td>-0.071853</td>\n",
              "      <td>0.122400</td>\n",
              "      <td>0.246660</td>\n",
              "      <td>-0.122470</td>\n",
              "      <td>0.248140</td>\n",
              "      <td>-0.153350</td>\n",
              "      <td>0.152790</td>\n",
              "      <td>0.929340</td>\n",
              "      <td>0.057485</td>\n",
              "      <td>0.161260</td>\n",
              "      <td>0.230990</td>\n",
              "      <td>0.238820</td>\n",
              "      <td>0.450830</td>\n",
              "      <td>0.219870</td>\n",
              "      <td>-0.58242</td>\n",
              "      <td>0.254090</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>0.317270</td>\n",
              "      <td>0.017905</td>\n",
              "      <td>-0.093428</td>\n",
              "      <td>-0.191410</td>\n",
              "      <td>-0.020396</td>\n",
              "      <td>-0.486050</td>\n",
              "      <td>0.27129</td>\n",
              "      <td>-0.61117</td>\n",
              "      <td>0.149090</td>\n",
              "      <td>0.121830</td>\n",
              "      <td>0.669370</td>\n",
              "      <td>-0.018887</td>\n",
              "      <td>-0.091606</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083122</td>\n",
              "      <td>-0.22120</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.199290</td>\n",
              "      <td>0.415630</td>\n",
              "      <td>-0.168280</td>\n",
              "      <td>-0.054537</td>\n",
              "      <td>0.437520</td>\n",
              "      <td>0.005782</td>\n",
              "      <td>0.057408</td>\n",
              "      <td>-0.303010</td>\n",
              "      <td>0.271360</td>\n",
              "      <td>0.303070</td>\n",
              "      <td>-0.041741</td>\n",
              "      <td>0.56293</td>\n",
              "      <td>0.062314</td>\n",
              "      <td>-1.6413</td>\n",
              "      <td>-0.23018</td>\n",
              "      <td>1.74480</td>\n",
              "      <td>-0.205090</td>\n",
              "      <td>0.225210</td>\n",
              "      <td>-0.180140</td>\n",
              "      <td>-0.129580</td>\n",
              "      <td>-0.431660</td>\n",
              "      <td>0.650840</td>\n",
              "      <td>-0.172870</td>\n",
              "      <td>0.097956</td>\n",
              "      <td>0.395640</td>\n",
              "      <td>0.52575</td>\n",
              "      <td>0.394430</td>\n",
              "      <td>-0.444890</td>\n",
              "      <td>0.103510</td>\n",
              "      <td>0.228380</td>\n",
              "      <td>0.196760</td>\n",
              "      <td>0.163680</td>\n",
              "      <td>0.733660</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>-0.008545</td>\n",
              "      <td>-0.938440</td>\n",
              "      <td>0.60734</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                1         2         3    ...       298       299      300\n",
              "according -0.279610  0.137320  0.043514  ...  0.390080 -0.324610 -0.01826\n",
              "several   -0.278830  0.298990 -0.092609  ... -0.224160 -0.366290 -0.21762\n",
              "court      0.294920 -0.274790 -0.616680  ... -0.207100  0.015589  0.17391\n",
              "say        0.056090  0.200350 -0.136570  ... -0.501320 -0.021014  0.44665\n",
              "around    -0.469570  0.000895 -0.030859  ...  0.184050 -0.110230 -0.31083\n",
              "foreign   -0.095487 -0.209960 -0.199050  ...  0.043425 -0.902620  0.49778\n",
              "10        -0.441890  0.242480 -0.229970  ... -0.776380 -0.054478 -0.13019\n",
              "until      0.045036 -0.162020  0.401750  ... -0.506100  0.070201  0.31148\n",
              "set       -0.330680  0.174590 -0.325450  ... -0.400650  0.052764  0.54662\n",
              "political  0.561610 -0.114750  0.421820  ... -0.008545 -0.938440  0.60734\n",
              "\n",
              "[10 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htf4Jj_GVGMn"
      },
      "source": [
        "#### For later processing¶\n",
        "- 'sos' token to be at index = 0\n",
        "- 'eos' token to be at index = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCwWH7wUVja1"
      },
      "source": [
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeX20qB3V5Mu"
      },
      "source": [
        "#### Swapping 'sos' token index and 'eos' token index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwRGHmfXV7Hz"
      },
      "source": [
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ImKSH4WvOj"
      },
      "source": [
        "#### Creating Sorted Instance of word2idx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPWum1oWgVk"
      },
      "source": [
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAY3toaqDxXA"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
        "\n",
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` which will be used to replace rare words later.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b71WvS0-X7Xu"
      },
      "source": [
        "#### Creating 2 Seperate Classes for Input Lang and Ouput Lang\n",
        "\n",
        "Class 1 - > InputLang - built from Glove\n",
        "\n",
        "Class 2 - > OutputLang - built from Target Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-xmITKdYD90"
      },
      "source": [
        "class InputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "        self.word2count = { word : 1 for word in words }\n",
        "        self.index2word = { i : word for word, i in word2idx.items() }\n",
        "        self.n_words = 400001\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ngtfyUqa-tm"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class OutputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLdDKLAODduG"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFrQoWJbEOYd"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696X0f6eEMa9"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVorNRFCETgM"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD5V5PPHERbk"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = InputLang(lang2)\n",
        "        output_lang = OutputLang(lang1)\n",
        "    else:\n",
        "        input_lang = InputLang(lang1)\n",
        "        output_lang = OutputLang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjaZPWj-Ed1p"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpLh4takEcl9"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQpC4FE3EsXQ"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQl0eg8BH9BS",
        "outputId": "c153f1d6-9cf9-4c40-9793-dcd6c760b3d6"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400005\n",
            "fra 4345\n",
            "['i m allergic to seafood .', 'je suis allergique aux fruits de mer .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Zbiow8b0rZ"
      },
      "source": [
        "#### Initializing weight Matrix\n",
        "\n",
        "We must build a matrix of weights that will be loaded into the PyTorch embedding layer. Its shape will be equal to:\n",
        "\n",
        "**(dataset’s vocabulary length, word vectors dimension)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC6WzLQScAG5"
      },
      "source": [
        "matrix_len = input_lang.n_words\n",
        "glove_dim = 300\n",
        "\n",
        "weights_matrix = np.zeros((matrix_len, glove_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(glove_dim, ))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IszO9MwEhLRe",
        "outputId": "78fe50c8-304c-4d9f-dbe8-307b16c707fe"
      },
      "source": [
        "weights_matrix.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400005, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sklKeEAZEw9A"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJZ7qgeoGH25"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtVkjGRfe-Pb"
      },
      "source": [
        "# def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "#     num_embeddings, embedding_dim = weights_matrix.size()\n",
        "#     emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "#     emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "#     if non_trainable:\n",
        "#         emb_layer.weight.requires_grad = False\n",
        "\n",
        "#     return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUFStkIuEqo7"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, weights_matrix=None):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        if weights_matrix is None:\n",
        "          self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WzcqRwLGR_c"
      },
      "source": [
        "#Simple Decoder\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTpja5ExGQR_"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMxRg_-Ga5T"
      },
      "source": [
        "# Attention Decoder\n",
        "\n",
        "If only the context vector is passed between the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        "![image](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTgpqalYGaOR"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xa7siQKGqQE"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mj3FhJBGoS_"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rtejHTjG5Ia"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained network is exploited, it may exhibit instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWVqaIu-Gt3b"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQi5ocOHDuv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A3z6eQtG3rv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po1iCyBTHHIG"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gf1aSD1HFx_"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxLPtvy-HJv8"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kErPEK3PHIjD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BWg0QVLHMnS"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjgrd4yHLMS"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDyINR3HP8X"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41NOzzmRHOfs"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I3YLTtFHSkq"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-45b8_lHRiK",
        "outputId": "321d4e59-f34d-4731-9861-27f4b8b79040"
      },
      "source": [
        "hidden_size = 300 #256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, weights_matrix).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6m 25s (- 90m 2s) (5000 6%) 3.4165\n",
            "12m 49s (- 83m 19s) (10000 13%) 2.7539\n",
            "19m 11s (- 76m 47s) (15000 20%) 2.4227\n",
            "25m 33s (- 70m 16s) (20000 26%) 2.1102\n",
            "31m 57s (- 63m 54s) (25000 33%) 1.8932\n",
            "38m 22s (- 57m 33s) (30000 40%) 1.7109\n",
            "44m 46s (- 51m 9s) (35000 46%) 1.5499\n",
            "51m 9s (- 44m 45s) (40000 53%) 1.4358\n",
            "57m 31s (- 38m 21s) (45000 60%) 1.2907\n",
            "63m 53s (- 31m 56s) (50000 66%) 1.2183\n",
            "70m 17s (- 25m 33s) (55000 73%) 1.1049\n",
            "76m 41s (- 19m 10s) (60000 80%) 1.0371\n",
            "83m 6s (- 12m 47s) (65000 86%) 0.9891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_rlcrR1Iqld",
        "outputId": "08f815bc-bce7-4d8e-fa78-924326e129bc"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> you re not as smart as me .\n",
            "= vous n etes pas aussi malignes que moi .\n",
            "< tu n es pas aussi malins que moi . <EOS>\n",
            "\n",
            "> you re all mine .\n",
            "= vous etes tous a moi .\n",
            "< tu es toutes a moi . <EOS>\n",
            "\n",
            "> you re very rude .\n",
            "= tu es fort grossier .\n",
            "< tu es tres grossieres . <EOS>\n",
            "\n",
            "> i am leaving next week .\n",
            "= je pars la semaine prochaine .\n",
            "< je pars la semaine . <EOS>\n",
            "\n",
            "> they are russian .\n",
            "= elles sont russes .\n",
            "< ils sont russes . <EOS>\n",
            "\n",
            "> i m not in the mood .\n",
            "= je ne suis pas d humeur .\n",
            "< je ne suis pas d humeur . <EOS>\n",
            "\n",
            "> i am grateful for your help .\n",
            "= je te suis reconnaissant pour ton aide .\n",
            "< je vous reconnaissant pour ton aide . <EOS>\n",
            "\n",
            "> i m sick of this hot weather .\n",
            "= je ne supporte plus ce temps chaud .\n",
            "< je ne fatigue plus des temps . <EOS>\n",
            "\n",
            "> they re able students .\n",
            "= ce sont des etudiants capables .\n",
            "< ce sont des etudiants . <EOS>\n",
            "\n",
            "> he is too shy to talk to girls .\n",
            "= il est trop timide pour parler aux filles .\n",
            "< il est trop pour pour a pour parler dur .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4J1zvrwInJk"
      },
      "source": [
        "Visualizing Attention\n",
        "---------------------\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "1t-jIrj1HVL9",
        "outputId": "8eae4654-c863-40fa-aa60-138088b1a161"
      },
      "source": [
        "%matplotlib inline\n",
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2a50864f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zYmdIDIvbR"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6iRp2UgrIxPQ",
        "outputId": "e3c9196a-62c7-4980-c20b-ec5b5e8b9953"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"he is painting a picture .\")\n",
        "\n",
        "evaluateAndShowAttention(\"why not try that delicious wine ?\")\n",
        "\n",
        "evaluateAndShowAttention(\"she is not a poet but a novelist .\")\n",
        "\n",
        "evaluateAndShowAttention(\"you re too skinny .\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = he is painting a picture .\n",
            "output = il est en train de peindre un tableau . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = why not try that delicious wine ?\n",
            "output = ce ne est pas si de cette ? <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = she is not a poet but a novelist .\n",
            "output = elle n est pas medecin mais mais . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = you re too skinny .\n",
            "output = vous etes trop maigrichonne . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM_7deQ7PB2u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}